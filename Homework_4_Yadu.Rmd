---
title: "Untitled"
author: "Yadu"
date: "November 5, 2016"
output: html_document
---

```{r}
library(stringr)
library(dplyr)
imputedDfTr <- read.csv("https://raw.githubusercontent.com/ChristopheHunt/DATA-621-Group-1/master/Homework%204/imputed_insurance_training_data.csv")
imputedDfTr <- subset(imputedDfTr, select = -INDEX)
```

```{r}
removedollars <- function(column){
             as.numeric(gsub("\\,","",gsub("\\$","", column)))
}
```

```{r}
imputedDfTr <- imputedDfTr %>%
        mutate(INCOME = removedollars(INCOME),
               HOME_VAL = removedollars(HOME_VAL),
               BLUEBOOK = removedollars(BLUEBOOK),
               OLDCLAIM = removedollars(OLDCLAIM),
               MSTATUS = gsub("z_", "", MSTATUS),
               URBANICITY = gsub("z_", "", URBANICITY),
               EDUCATION = gsub("z_", "", EDUCATION),
               EDUCATION = gsub("<", "Less Than ", EDUCATION),
               JOB = gsub("z_", "", JOB),
               CAR_TYPE = gsub("z_", "", CAR_TYPE),
               SEX = gsub("z_", "", SEX))
```

```{r}
winsorize <- function(x, multiple=2.2)
{
  q <- quantile(x)
  iqr <- IQR(x)
  iqrAdjusted <- iqr*multiple

  rangeLow <- q['25%']-iqrAdjusted
  rangeHigh <- q['75%']+iqrAdjusted

  x[x<rangeLow] <- min(x[x>rangeLow])
  x[x>rangeHigh] <- max(x[x<rangeHigh])

  return(x)  

}
library(forecast)

wimputedDfTr <- cbind(sapply(c('INCOME', 'HOME_VAL', 'OLDCLAIM', 'BLUEBOOK'), function(x) winsorize(imputedDfTr[,x], multiple=2.2)), subset(imputedDfTr, select = -c(INCOME, HOME_VAL, OLDCLAIM, BLUEBOOK)))
#library(stargazer)
#stargazer(wimputedDfTr, header = FALSE)
```

```{r}
library(car)
fit <- lm(TARGET_AMT ~ KIDSDRIV + AGE + HOMEKIDS + YOJ + INCOME + PARENT1 + HOME_VAL + MSTATUS + SEX + EDUCATION + JOB + TRAVTIME + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + RED_CAR + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + URBANICITY, data=wimputedDfTr)
ncvTest(fit, ~ KIDSDRIV + AGE + HOMEKIDS + YOJ + INCOME + PARENT1 + HOME_VAL + MSTATUS + SEX + EDUCATION + JOB + TRAVTIME + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + RED_CAR + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + URBANICITY, data=wimputedDfTr)
```

```{r}
residualPlots(fit)

```

```{r}
l1 = BoxCox.lambda(wimputedDfTr$INCOME)
l2 = BoxCox.lambda(wimputedDfTr$HOME_VAL)
l3 = BoxCox.lambda(wimputedDfTr$TRAVTIME)
```

The Box-Cox transformations were done only on three of the input variables - income, house value, and the total number of claims during the past 5 years. These transformations were done based on the residual plots. In the residual plots, these three variables showed a great deal of non-constant variance because the plots were funnel-shaped. 

MODEL SELECTION

Out of all the logistic models, the Subset Selection Model rated the best due to the highest accuracy. The AIC for this model was not the lowest. However, the difference between this AIC (7708.37) and the lowest AIC (7703.13) is not too much.

In the linear models rendered, some of the correlation coefficients do make sense. For example, in both models, it would make sense for a student to have to pay less in a car crash due to the fact that the student would spend more time studying and less time on the road. It would also make sense for urbanicity to affect the cost during a car crash because in urban areas, there are more people and pedestrians and the probability of people getting killed in a car crash would be very high as opposed to rural areas where there would be less pedestrians.   

MODELS BUILT

In the backwards selection model, the resulting AIC was 7,703.13. All of the variables remain except the age of the driver, sex, whether the car was red or not, number of kids at home, car use, and car age. Zero correlation for bluebook value is a result of our transformations. As the bluebook value increases, the log likelihood that the vehicle gets into a crash decreases.

In the forwards selection model, the resulting AIC was 7,712.07. All of the variables remain in the model. 

In the subset selection model, the resulting AIC was 7,708.37. Very few of the variables remained. These variables were number of kids driving, income, marital status, job category, car use, whether the license was revoked or not, urbanicity, and motor vehicle record points. In this model, the target variable has a stronger correlation with urbanicity than it does in the other logistic models. 

In the backwards linear regression model, the variables eliminated were the number of years on the job, whether the car is red or not, number of kids at home, total claims within the past 5 years, and age of the driver. The target variable has the strongest correlation with the urbanicity. Due to our transformation of the income variable, income does have some correlation now with the target amount. Before our tranformation there was no correlation at all between these two variables. 

In the forwards linear regression model, the variables eliminated were the same as the variables eliminated in the backwards linear regression. Just like the previous model, the target variable has the strongest correlation with urbanicity. 

After looking at the backwards selection model, we removed the variables that appeared to not be significant in the forwards selection model. The variables that we removed were bluebook, 

Correlation Matrix explanation

target flag in the third bullet has to be lower case.

elaborate more on the forwards selection model.

In the subset selection model, the AIC is not the lowest. However, it is a simpler model with a better precision. This is why we chose this model over the other two. 