---
title: "Homework 1"
author: "Group 1"
date: ''
output:
  pdf_document:
    includes:
      in_header: header.tex
    number_sections: yes
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: '2'
---

\begin{center}
\bigskip
\bigskip
\bigskip
Prepared for:\\
\medskip
Dr. Nathan Bastian\\
\smallskip
City University of New York, School of Professional Studies - Data 621\\
\bigskip
Prepared by:\\
\medskip
Group 1\\ 
\medskip
Senthil Dhanapal\\ 
\smallskip
Yadu Chittampalli\\
\smallskip
Christophe Hunt\\  

\end{center}

```{r, include = FALSE}
library(pacman)
p_load(Hmisc, xlsx, xtable, knitr, scales, magrittr, tidyr, plyr, dplyr, 
       stringr, e1071, corrplot, knitcitations, bibtex, missForest,
       foreach, doParallel, stargazer, forecast)
```

```{r, include=FALSE, cache=TRUE}
download.file("https://github.com/ChristopheHunt/DATA-621-Group-1/blob/master/Data%20Dictionary.xlsx?raw=true", 
              destfile = "Data Dictionary HW1 - Group 1.xlsx", mode='wb')
data_dictionary <- read.xlsx("Data Dictionary HW1 - Group 1.xlsx", sheetIndex = 1)
file.remove(dir(getwd(), pattern = "Data Dictionary HW1 - Group 1", full.names = TRUE))
```
\newpage


# Introduction

The ability to analyize and predict performance of a professional baseball team using many dimensions is critical to competitive success for our organization. Therefore, we have analyzed the records of numerous professional baseball team from the years 1871 to 2006. Our hope is that the following report and the resulting predictive models will better inform the organization and assist in making data driven decisions moving forward. 

"The goal of a baseball team is to win more games than any other team. Since one team has very little control over the number of games other teams win, the goal is essentially to win as many games as possible. Therefore, it is of interest to measure the player's contribution to the team's wins." Grabiner, B. D. [^1] While we do not have the variables at the player's individual contribution level, we do have the entire teams contributions as an aggregate and will analyze that information. 

[^1]: (Grabiner, B. D. (n.d.). [The Sabermetric Manifesto.](http://seanlahman.com/baseball-archive/sabermetrics/sabermetric-manifesto/) Retrieved September 10, 2016 from http://seanlahman.com/baseball-archive/sabermetrics/sabermetric-manifesto/) 

# Statement of the Problem

The purpose of this report is to determine the batting, baserun, pitching, and fielding effects on a baseball team's ability to win.  

# Data Exploration  

Note that each record has the performance of the team for the given year, with all of the statistics adjusted to match the performance of a 162 game season. The following Table 1 - Descriptive Statistics provides the detailed descriptive statistics regarding our variable of interest - Number of Wins and our possible explanatory variables. 

We noted that several variables were missing a nontrivial amount of observations and these variables are Strikeouts by batters, Stolen Bases, Caught stealing, Batters hit by pitch (get a free base), Strikeouts by pitcher, and Double plays. So we will need to address the missing values for further analysis. 

Histograms of all of the variables have been plotted below so that the distribution of the data can be visualized. In the distribution for the number of walks allowed, only two bars exist due to the excessive number of outliers.

Additionally, the skewness of each variable has been indicated in Table 1 - Descriptive Statistics. Several variables have a significant amount of skew, which include the number of base hits by batters and the number of walks allowed. Correspondingly, these two variables had a skew of 1.57 and 6.74 respectively. 

```{r, echo=FALSE, results='asis', cache=TRUE}
df <- read.csv("https://raw.githubusercontent.com/ChristopheHunt/DATA-621-Group-1/master/moneyball-training-data.csv")

colnames(df) <- mapvalues(as.vector(colnames(df)), 
                            from = str_trim(data_dictionary$VARIABLE.NAME..), to = as.vector(str_trim(data_dictionary$DEFINITION)))

digits <- c(1) 

descriptive <- describe(df %>% select(-`Identification Variable (do not use)`), 
                        descript = "Table 1 : Descriptive Statistics", digits = digits) 

for (i in seq(1:length(descriptive))){ 
                     for (j in c(6,9,12)){names(descriptive[[i]]$counts)[j] <- paste0(names(descriptive[[i]]$counts)[j]," freq")
                                         }
     descriptive[[i]]$counts <- (descriptive[[i]]$counts[-c(4,7,8:10,11)])
     
     descriptive[[i]]$counts[7] <- round(sapply(df[i + 1], function(x) median(x, na.rm = TRUE)), digits = digits)
     names(descriptive[[i]]$counts)[7] <- "Median"
     
     descriptive[[i]]$counts[8] <- round(sapply(df[i + 1], function(x) sd(x, na.rm = TRUE)), digits = digits)
     names(descriptive[[i]]$counts)[8] <- "SD"
     
     descriptive[[i]]$counts[9] <- round(sapply(df[i + 1], function(x) skewness(x, na.rm = TRUE)), digits = digits)
     names(descriptive[[i]]$counts)[9] <- "Skew"
     
     descriptive[[i]]$counts <- (descriptive[[i]]$counts[c(1:4,7:9,5:6)]) #reorder 
}

latex(descriptive, file = '')
```

## Imputing Missing Values  

In order to address the missing values in our variables we used a nonparametric imputation method (Random Forest) to impute missing values. We chose a nonparametric method due to several variables having significant skew and greater than expected kurtosis values. \newpage 

```{r, results='asis', echo = FALSE, include=FALSE, cache=TRUE, eval=FALSE}
registerDoParallel(cl = makeCluster(2), cores = 3)

set.seed(1234)

imputed_data <- df %>% 
              select(-`Identification Variable (do not use)`) %>%
              missForest(maxiter = 10, ntree = 100, parallelize = 'forests')

write.csv(imputed_data$ximp,"imputed-moneyball-training-data.csv", row.names = FALSE)
```

```{r, include=FALSE, cache=TRUE}
## to ensure similar results we load the imputed data from github 
imputed_data <- read.csv("https://raw.githubusercontent.com/ChristopheHunt/DATA-621-Group-1/master/imputed-moneyball-training-data.csv", header = TRUE, check.names = FALSE)
```

## Correlation Matrix 

After competing the imputation, we can implement a correlation matrix to better understand the correlation between variables in the data set. The below matrix is the results and as expected, Number of Wins appears to be most correlated to Base Hits by batters (1B,2B,3B,HR). 

```{r, fig.cap= "Correlation Plot of Training Data Set with imputed values", echo = FALSE, cache=TRUE}
imputed_df_m <- as.matrix(imputed_data)
cor_matrix <- cor(imputed_df_m, use = "everything",  method = c("pearson"))
corrplot(cor_matrix, order = "hclust", addrect = 2, method="square", tl.col = "black", tl.cex = .5, na.label = " ")
```

\newpage

# Data Preparation  

First, we chose to eliminate two variables that had a significant number of missing data points. These variables were Batters hit by pitch (get a free base) and Caught stealing, which were missing `r percent(2085/2276)` and `r percent(772/2276)` respectively. 

```{r, include = FALSE, cache=TRUE}
imputed_df <- imputed_data %>% select(-`Batters hit by pitch (get a free base)`, -`Caught stealing`)
```

Additionally, we reduced the data set to the following variables for modeling simplicity. Base Hits by batters (1B,2B,3B,HR), Strikeouts by batters, Walks by batters

Missing values in the remaining columns had been imputed using the random forest method as previous discussed in section 3.1. 

## Outliers 

Winsorizing - 

```{r, echo=FALSE, include = FALSE}
winsorize <- function(x, multiple=2.2)
              {
                q <- quantile(x)
                iqr <- IQR(x)
                iqrAdjusted <- iqr*multiple
                
                rangeLow <- q['25%'] - iqrAdjusted
                rangeHigh <- q['75%'] + iqrAdjusted
                
                x[x<rangeLow] <- min(x[x > rangeLow])
                x[x>rangeHigh] <- max(x[x < rangeHigh])
                
                return(x)  
}

for (i in 1:length(imputed_df)){
  imputed_df[i] <- sapply(imputed_df[i], function(x) winsorize(x))
} 

imputed_df <- as.data.frame(do.call(cbind, imputed_df))
```

## Box Cox Transformation

We choose the Box Cox transformation for the following variables to improve linearity in our model. 

```{r, echo=FALSE, cache=TRUE, results = 'asis'}

l1 <- BoxCox.lambda(imputed_df$`Base Hits by batters (1B,2B,3B,HR)`)

imputed_df$`Base Hits Transformed` <-  BoxCox(imputed_df$`Base Hits by batters (1B,2B,3B,HR)`, l1)

hist(imputed_df$`Base Hits Transformed`, 
     main = ("Histogram of Base Hits After Box Cox Transformation"), 
     xlab = ("Box Cox Transformation of Base Hits"))

imputed_df$`Walks Allowed Transformed` <- (imputed_df$`Walks allowed` ^ (1/3))

hist(imputed_df$`Walks Allowed Transformed`, 
     main = ("Histogram of Cube Root of Walks Allowed"), 
     xlab = ("Cube Root of Walks Allowed"))

skewness(imputed_df)
```

The variables that were transformed were the number of walks allowed and the number of base hits by batters. The cube root of the number of walks allowed was taken because the skewness in this variable was high. After the cube root was taken, the skewness was reduced to approximately 0.5.

The transformation done on the number of walks was Box Cox transformation. As a result of this transformation the skewness was reduced to 0.112. 

Now the distributions are fairly symmetrical.

# Models Built

## Model 1 
 
We used the variable Base Hits by batters (1B,2B,3B,HR) which is the most correlated variable to Number of Wins as indicated in the correlation matrix. This is expected as Base Hits are necessary to win any game. Walks by batters was added as walks indicate that a batter is Additionally, Strikeouts by batters would be negatively correlated Number of Wins because if a batter strikes out they are no able to provide runs which are criticaly to win. 



```{r, results='asis', echo=FALSE}
lmfit_data <- imputed_df

lmfit_data$`Sqrt of Strikeouts by batters` <- sqrt(imputed_df$`Strikeouts by batters`)

lmfit <- lm(data = lmfit_data, 
           `Number of wins` ~ `Base Hits by batters (1B,2B,3B,HR)` +  
                               `Walks by batters` + 
                               `Sqrt of Strikeouts by batters` )

stargazer(lmfit, header = FALSE,  
          covariate.labels = c("Base Hits by batters (1B,2B,3B,HR)", 
                               "Walks by batters", 
                               "$\\sqrt{\\textnormal{Strikeouts~by~batters}}$"),
          dep.var.caption  = c("Model 1"),
          dep.var.labels   = c("Number of wins"))

```

In the previous model, we see an interesting Constant of `r round(lmfit$coefficients[[1]],2)`, since it would be impossible for a team to have negative wins. Additionally, the coefficient for $\sqrt{\textnormal{Strikeouts~by~batters}}$ at `r round(lmfit$coefficients[[4]],4)` is surprisingly as we would expect Strikeouts by batters to actually redce the number of wins. 

\newpage

# Selected Model 

```{r}
eval_data <- read.csv("https://raw.githubusercontent.com/ChristopheHunt/DATA-621-Group-1/master/moneyball-evaluation-data.csv")

colnames(eval_data) <- mapvalues(as.vector(colnames(eval_data)), 
                            from = str_trim(data_dictionary$VARIABLE.NAME..), 
                            to = as.vector(str_trim(data_dictionary$DEFINITION)))
eval_data <- eval_data %>%
             mutate(`Sqrt of Strikeouts by batters` =  sqrt(`Strikeouts by batters`))
```

Decide on the criteria for selecting the best multiple linear regression model. Will you select a model with slightly
worse performance if it makes more sense or is more parsimonious? Discuss why you selected your model.
For the multiple linear regression model, will you use a metric such as Adjusted R2
, RMSE, etc.? Be sure to
explain how you can make inferences from the model, discuss multi-collinearity issues (if any), and discuss other
relevant model output. Using the training data set, evaluate the multiple linear regression model based on (a)
mean squared error, (b) R2
, (c) F-statistic, and (d) residual plots. Make predictions using the evaluation data 

Using our best performing model we used the `predict` function and excluded evaluation data with missing values. The below table is our prediction results for the evaluation data set. 

```{r}
eval_data$predictions <- predict(lmfit, eval_data)

eval_results <- eval_data %>%
                filter(!is.na(predictions)) %>%
                select(`Identification Variable (do not use)`, predictions)

xtable(eval_results)
```


```{r, echo=FALSE, results='asis'}
stargazer::stargazer(lmfit)
```

\newpage

# Appendix A

## Session Info

```{r, results='asis', echo=FALSE}
toLatex(sessionInfo())
```

## Citations

## Data Dictionary

```{r, results='asis', echo=FALSE, cache=TRUE}
kable(data_dictionary)
```

## R source code
