---
title: "Homework 3"
author: "Group 1"
date: ''
output:
  pdf_document:
    includes:
      in_header: header.tex
    number_sections: yes
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: '2'
---

```{r programming style guide, echo=FALSE, eval=FALSE}
##Style guide
##USE CAMEL CASING!! https://en.wikipedia.org/wiki/Camel_case, start with lowercase then each word after start with upper case
##upper case for vector, lists, etc. ex. X <- c("yes","no","true","false")
##lower case for scalar/single value, ex. x <- 1
##data frame start with df followed by description, ex. dfEval <- evaluation data frame
##assignments use "<-" not "="
```

```{r library load, include = FALSE}
library(pacman)
p_load(Hmisc, xlsx, xtable, knitr, scales, magrittr, tidyverse, stringr, e1071, corrplot, knitcitations, bibtex, missForest, abc,
       foreach, doParallel, stargazer, forecast, matrixStats, glmulti, leaps, data.table, highlight, car)
```

\begin{center}
\bigskip
\bigskip
\bigskip
Prepared for:\\
\medskip
Dr. Nathan Bastian\\
\smallskip
City University of New York, School of Professional Studies - Data 621\\
\bigskip
Prepared by:\\
\medskip
Group 1\\ 
\medskip
Senthil Dhanapal\\ 
\smallskip
Yadu Chittampalli\\
\smallskip
Christophe Hunt\\  

\end{center}

```{r data dictionary load, include=FALSE, cache=TRUE}
download.file("https://github.com/ChristopheHunt/DATA-621-Group-1/raw/master/Homework%203/Data%20Dictionary.xlsx?raw=true", destfile = "Data Dictionary HW3 - Group 1.xlsx", mode = 'wb')
daDict <- read.xlsx("Data Dictionary HW3 - Group 1.xlsx", sheetIndex = 1)
file.remove(dir(getwd(), pattern = "Data Dictionary HW3 - Group 1", full.names = TRUE))
```


\newpage

# Introduction

Crime has a high cost to all parts of society and it can have severe long term impact on neighborhoods. If crime rises in the neighborhood or it is invaded by criminals, then families and those with the economic means to leave for more stable areas will do so[^1]. Additionally, crime can even have a health cost to the community in that the perception of a dangerous neighborhood was associated with significantly lower odds of having high physical activity among both men and women[^2]. It is important to understand the propensity for crime levels of a neighborhood before investing in that neighborhood. 

[^1]: Effect of Crime on Real Estate Values. (1952). The Journal of Criminal Law, Criminology, and Police Science, 43(3), 357-357. Retrieved from http://www.jstor.org.remote.baruch.cuny.edu/stable/1139159

[^2]: Bennett GG, McNeill LH, Wolin KY, Duncan DT, Puleo E, Emmons KM (2007) [Safe To Walk? Neighborhood Safety and Physical Activity Among Public Housing Residents.](http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0040306) PLoS Med 4(10): e306. doi:10.1371/journal.pmed.0040306

# Statement of the Problem

The purpose of this report is to develop a statistical model to determine the variables that are independently associated with neighborhoods with crime rates above or below the median. Note that neighborhoods with crime rates above or below the median have already been provided in our evaluation data set.

# Data Exploration  

## Variables Explained

The variables provided in our evaluation data set our explained below:

```{r data dictionary table, eval=TRUE, results='asis', echo=FALSE}
kable(daDict %>% 
      filter(VARIABLE.TYPE == "Predictor Variable") %>%
      mutate(Abbreviation = VARIABLE.NAME, Definition = DEFINITION)  %>%
      dplyr::select(Abbreviation, Definition),
      align = c("c","l"))
```

\newpage

## Exploration of Variables

The skewness of each input variable is shown below. The two variables with the strongest skew are the proportion of residential land zoned for large lots and the proportion of blacks by town. Respectively the magnitudes of the skewness of these two variables are 2.18 and 2.92. This indicates that the distributions for these two variables are far from symmetrical. The skewness of the dummy variable (whether the suburb borders the river or not) can be neglected because it is a binary variable. All of the other variables skewnesses that are approximately of magnitude 1 or less. This indicates that the distributions for those variables can be considered symmetric even though for three of the variables (concentration of nitrogen oxides, index of accessibility to radial highways, and median value of owner-occupied homes) are multimodal.

```{r skew, echo=FALSE, results='asis'}
library(matrixStats)
crimes <- dfTr <- df <- read.csv("https://raw.githubusercontent.com/ChristopheHunt/DATA-621-Group-1/master/Homework%203/crime-training-data.csv")
dfSkew <- setDT(as.data.frame(abs(skewness(crimes))), keep.rownames = TRUE)[]
colnames(dfSkew) <- c("variables", "skew")
kable(dfSkew, align = c("l", "c"))
```

According to the standard deviations of each variable, the variable that has the highest difference from the mean is tax. 

```{r sd, echo=FALSE, results='asis'}
kable(data.frame(cbind(variables = colnames(crimes), sd = colSds(as.matrix(crimes)))), align = c("l", "c"))
```

\newpage

Histograms of most of our variables have been plotted below so that distribution can be visualized. We have excluded `target` and `chas` due to being binary and not being well represented in the below visualization. We also excluded `rad` as it is an index variable and also is not best represented in the below visualization. 


```{r descriptive statistics table, echo=FALSE, results='asis', cache=TRUE}
digits <- c(1)

descriptive <- describe( dfTr %>% dplyr::select(-target, -chas, -rad) 
                        , descript = "Table 1 : Descriptive Statistics", digits = digits)
latex(descriptive, file = '')
```

## Correlation Matrix 

We implement a correlation matrix to better understand the correlation between variables in the data set. The below matrix is the results and we noticed a few interesting correlations. 

* High nitrogen oxides concentration (parts per 10 million) ("nox") is positively correlated with higher than median crime rates. As defined by the EPA  - "NOx pollution is emitted by automobiles, trucks and various non-road vehicles (e.g., construction equipment, boats, etc.) as well as industrial sources such as power plants, industrial boilers, cement kilns, and turbines"[^4]. It is clear to see that nox is concentrated in areas of high road traffic and possible high industrial use which would be neighborhoods of low value and may attract crime. 

* The weighted mean of distances to five Boston employment centers is negatively correlated with a city with higher than median crime rate. This is intuitive in that employment centers would be more closely located in cities of high crime due to high unemployment being positively correlated with higher crimes rates[^5]. 

* The tax is positively correlated with higher than median crime rate which is counter intuitive because we would think as tax increases then crime would decrease (more valuable property = higher tax = less crime). 

* We also see bk is negatively correlated with higher than median crime rates but it seems to be due to the transformation of 1000(Bk - 0.63)^2. Further resources on why this type of transformation is being used were not available. It should be noted that this transformation causes a counter intuitive correlation. 

[^4]: ["Nitrogen Oxides Control Regulations | Ground-level Ozone | New England | US EPA."](https://www3.epa.gov/region1/airquality/nox.html) EPA. Environmental Protection Agency, n.d. Web. 22 Oct. 2016.

[^5]: Ajimotokin, S., Haskins, A., & Wade, Z. (2015). [The Effects of Unemployment on Crime Rates in the US.](https://smartech.gatech.edu/bitstream/handle/1853/53294/theeffectsofunemploymentoncimerates.pdf?sequence=1&isAllowed=y)


```{r correlation matrix, fig.cap= "Correlation Plot of Training Data Set", echo = FALSE, fig.height = 4, fig.width= 6, cache=TRUE}
dfTrMx <- as.matrix(dfTr)
corMx <- cor(dfTrMx , use = "everything",  method = c("pearson"))
corrplot(corMx, order = "hclust", addrect = 2, method = "square", tl.col = "black", tl.cex = .5, na.label = " ")
```

\newpage 

## Outliers Treatment

We chose winsorizing as the method to address outliers. Instead of trimming values, winsorizing uses the interquantile range to replace values that are above or below the interquantile range multiplied by a factor. Those values above or below the range multiplied by the factor are then replaced with max and min value of the interquantile range. Using the factor 2.2 for winsorizing outliers is a method developed my Hoaglin and Iglewicz and published Journal of American Statistical Association in 1987[^6].  

[^6]:Hoaglin, D. C., and Iglewicz, B. (1987), Fine tuning some resistant rules for outlier labeling, Journal of American Statistical Association, 82, 1147-1149.

The below table is the summary results of the winsorizing of the data. 

```{r winsorize function, echo=FALSE}
winsorize <- function(x, multiple=2.2)
{
  q <- quantile(x)
  iqr <- IQR(x)
  iqrAdjusted <- iqr*multiple

  rangeLow <- q['25%']-iqrAdjusted
  rangeHigh <- q['75%']+iqrAdjusted

  x[x<rangeLow] <- min(x[x>rangeLow])
  x[x>rangeHigh] <- max(x[x<rangeHigh])

  return(x)  

}
```

```{r winsorize dataframe,  echo=FALSE, results='asis'}
library(forecast)

winsorizedDataset <- function(dataset){
  dataset <- dataset[,1:ncol(dataset)]
  wdataset <- matrix(data = 0, nrow = nrow(dataset), ncol = ncol(dataset))
  #l1 <- numeric(ncol(dataset))
  for (i in 1:ncol(dataset)){
    if(i == 3 || i == ncol(dataset)){
      wdataset[,i] <- dataset[,i]
    }
    else {
    wdataset[,i] <- winsorize(dataset[,i], multiple = 2.2)
    #l1[i] <- BoxCox.lambda(wdataset[,i])
    }
    colnames(wdataset) <- colnames(dataset)
  }
  return(data.frame(wdataset))
}
#lambdas <- winsorizedandtransformed(crimes)[[2]]
#wtcrimes <- winsorizedandtransformed(crimes)[[1]]
wCrimes <- winsorizedDataset(crimes)

stargazer(wCrimes, header = FALSE)
```

\newpage

## BoxCox Transformations 

Using the `BoxCox.lambda` function from the `forecast` package we are able to determine our necessary transformations to our independent variables. 

```{r boxcox transformation, echo=FALSE, cache=TRUE, results='asis'}
boxcoxLambdas <- function(dataset){
  dataset <- dataset[,1:ncol(dataset)]
  #tdataset <- matrix(data = 0, nrow = nrow(dataset), ncol = ncol(dataset))
  l1 <- numeric(ncol(dataset))
  for (i in 1:ncol(dataset)){
    l1[i] <- BoxCox.lambda(dataset[,i])
  }
  return(data.frame(l1, colnames(dataset)))
}
dfBoxCox <- boxcoxLambdas(wCrimes %>% dplyr::select(-target))

colnames(dfBoxCox) <- c("$\\lambda$", "Variables")

kable(dfBoxCox, align = c("c", "c"))
```

Utilizing the below table of common transformations based on the lambda value of the BoxCox we further transform our independent variables.

\centering

Common Box-Cox Transformations[^7]

\setlength{\tabcolsep}{12pt}

\begin{tabular}{ c c }
\hline
$\lambda$ & Y' \\ \hline
-2 &	$Y^{-2}~=~\frac{1}{Y^{2}}$ \\
-1 &	$Y^{-1}~=~\frac{1}{Y^{1}}$ \\
-0.5 &	$Y^{-0.5}~=~\frac{1}{\sqrt{(Y)}}$) \\
0	& $\log(Y)$ \\
0.5	& $Y^{0.5}~=~\sqrt{(Y)}$ \\
1	& $Y^{1}~=~Y$ \\
2	& $Y^{2}$ \\
\end{tabular}

\justifying

Lambda values that did not fall in the proximity of common transformations were ignored. All other Lambda values were truncated to the nearest tenth that match a common transformation as per the below table.

\centering

\begin{tabular}{ c c }
\hline
variable & variable transformation \\ \hline
indus & $\log{indus}$ \\
chas & $\sqrt{chas}$ \\
nox & $nox^{-1}$ \\
rm & $\log{rm}$ \\
age & $age^{2}$ \\
dis & $dis^{-.5}$ \\
tax & $tax^{-1}$ \\
ptratio & $ptratio^{2}$ \\
black & $black^{2}$ 
\end{tabular}

\justifying

\setlength{\tabcolsep}{6pt}

[^7]: [By Understanding Both the Concept of Transformation and the Box-Cox Method, Practitioners Will Be Better Prepared to Work with Non-normal Data.](https://www.isixsigma.com/tools-templates/normality/making-data-normal-using-box-cox-power-transformation/) . "Making Data Normal Using Box-Cox Power Transformation." ISixSigma. N.p., n.d. Web. 29 Oct. 2016.

\newpage

# Models Built

```{r model metrics, echo=FALSE}
#Supporting Functions

TP <- function(df, predResp, resp, p, n)
{
  return(length(df[which(df[predResp]==p & df[resp]==p),1]))
}

TN <- function(df, predResp, resp, p, n)
{
  return(length(df[which(df[predResp]==n & df[resp]==n),1]))
}

FP <- function(df, predResp, resp, p, n)
{
  return(length(df[which(df[predResp]==p & df[resp]==n),1]))
}

FN <- function(df, predResp, resp, p, n)
{
  return(length(df[which(df[predResp]==n & df[resp]==p),1]))
}


Accuracy <- function(df, predResp, resp, p, n)
{
  tp <- TP(df,predResp,resp,p,n)
  tn <- TN(df,predResp,resp,p,n)
  fp <- FP(df,predResp,resp,p,n)
  fn <- FN(df,predResp,resp,p,n)
  
  return ((tp + tn)/(tp + fp + tn + fn))
  
}

ClassificationErrorRate <- function(df, predResp, resp, p, n)
{
  tp <- TP(df,predResp,resp,p,n)
  tn <- TN(df,predResp,resp,p,n)
  fp <- FP(df,predResp,resp,p,n)
  fn <- FN(df,predResp,resp,p,n)
  
  return ((fp + fn)/(tp + fp + tn + fn))
  
}

Precision <- function(df, predResp, resp, p, n)
{
  tp <- TP(df,predResp,resp,p,n)
  fp <- FP(df,predResp,resp,p,n)
  
  return (tp/(tp + fp))
  
}

Sensitivity <- function(df, predResp, resp, p, n)
{
  tp <- TP(df,predResp,resp,p,n)
  fn <- FN(df,predResp,resp,p,n)
  
  return (tp/(tp + fn))
  
}

Specificity <- function(df, predResp, resp, p, n)
{
  tn <- TN(df,predResp,resp,p,n)
  fp <- FP(df,predResp,resp,p,n)
  
  return (tn/(tn + fp))
  
}

F1Score <- function(df, predResp, resp, p, n)
{
  precision <- Precision(df,predResp,resp,p,n)
  sensitivity <- Sensitivity(df,predResp,resp,p,n)
  
  return ((2 * precision * sensitivity)/(precision + sensitivity))
  
}


ROC <- function(df, respProb, resp, p, n, thresholds, plotTitle)
{
  x <- thresholds
  dfForPlot <- data.frame(threshold=integer(), fpr=double(),tpr=double(), auc=double(), dist=double())
  for(i in x)
  {
    plotCoord <- ROC.Coordinates(df,resp, respProb, p, n, i)
    #print(plotCoord)
    dfForPlot <-  data.frame(rbind(dfForPlot,data.frame(plotCoord)))
  }
  plot(x=dfForPlot$fpr,y=dfForPlot$tpr,lwd=2, type="l", xlab="FPR", ylab="TPR", main=plotTitle)
  lines(x=c(0, 1), y=c(0, 1), col="black", lwd=1)
  return (dfForPlot)
  
}


ROC.Coordinates <- function(df, resp, respProb, p, n, threshold)
{
  pred <- rep(n,length(df[,1]))
  dfNew <- df
  pred[which(df[respProb]>=threshold)]  <- p
  dfNew$pred <- as.factor(pred)
  
  sensitivity <- Sensitivity(df = dfNew, predResp = "pred",resp = resp,p = p,n = n)
  specificity <- Specificity(df = dfNew, predResp = "pred",resp = resp,p = p,n = n)
  
  tpr <- sensitivity
  fpr <- 1 - specificity
  auc <- (sensitivity + specificity)/2
  dist <- sqrt((1-tpr)^2 + (fpr)^2)
  
  return (data.frame(threshold,fpr,tpr,auc,dist))
  
}

calculateAUC <- function(curveInfo)
{
  tpr1 <- 0
  fpr1 <- 0
  auc <- 0
  curveInfo <- curveInfo[order(-curveInfo$threshold),]
  for(i in 1:nrow(curveInfo))
  {
    tpr2 <- curveInfo[i,"tpr"]
    fpr2 <- curveInfo[i,"fpr"]
    
    auc <- auc + ((tpr1 + tpr2)/2)*(fpr2-fpr1)
    
    
    tpr1 <- tpr2
    fpr1 <- fpr2
  }
  return(auc)
}


ConfusionDataFrame <- function(tp,fp,tn,fn)
{
  cm <- data.frame( rbind(tp,fp), rbind(fn, tn))
  colnames(cm) <- c("Act-Pos","Act-Neg")
  rownames(cm) <- c("Pred-Pos","Pred-Neg")
  return(cm)
}

ModelMetrics <- function(df, predResp, resp, p, n, modelName, aic)
{
  acc <- Accuracy(dfPred,"predResp","target",1,0)
  cer <- ClassificationErrorRate(dfPred,"predResp","target",1,0)
  prec <- Precision(dfPred,"predResp","target",1,0)
  sens <- Sensitivity(dfPred,"predResp","target",1,0)
  spec <- Specificity(dfPred,"predResp","target",1,0)
  f1score <- F1Score(dfPred,"predResp","target",1,0)
  roc <- ROC(dfPred,"predProb","target",1,0, seq(0,1,.01),modelName)
  
  auc <- calculateAUC(roc)
  
  bestThreshold <- roc[which(roc$auc == max(roc$auc)),][1,][,1]

  
  tp <- TP(dfPred,"predResp","target",1,0)
  tn <- TN(dfPred,"predResp","target",1,0)
  fp <- FP(dfPred,"predResp","target",1,0)
  fn <- FN(dfPred,"predResp","target",1,0)
  
  print(ConfusionDataFrame(tp,fp,tn,fn))
  
  M <- format(c(acc,cer,prec,sens,spec,f1score,auc,bestThreshold,aic), digits = 3)
  df <- data.frame("metric" = M)
  r.n <- c("accuracy", "classif.error","precision","sensitivity","specificity","f1score","auc","best.threshold", "aic")
  rownames(df) <- r.n
  colnames(df) <- modelName
  
  return(df)
}
```

## Model 1 - Backwards Selection Method


```{r backwards model selection, echo=FALSE, results='asis'}
fullModel <- lm(target ~ zn + I(log(indus)) + I(sqrt(chas)) + I(nox^-1) + I(log(rm)) + I(age^2) + I(dis^-.5) + rad + I(tax^-1) + I(ptratio^2) + I(black^2) + lstat + medv, data = wCrimes) ##TODO - Review before submission
backFit <- fit <- glm(step(fullModel, direction = "backward", trace = F))
stargazer(fit, header = FALSE)
```

### Model Metrics

```{r backwards model metrics, echo=FALSE, results='asis', eval=FALSE}
dfPred <- df[,c("nox","age","dis","rad","tax","ptratio", "black","lstat", "medv")]

predProb <- predict(fit,dfPred,type="response") #Predicted Probability

predResp <- numeric(length(predProb)) #Predicted Class
predResp[which(predProb>=0.59)] <- 1

dfPred <- data.frame(cbind(df[,c("nox","age","dis","rad","tax","ptratio", "black","lstat", "medv")],predProb, predResp))


dfFwdModel <- ModelMetrics(dfPred,"predResp","target",1,0,"forward", fit$aic)
dfBwdModel <- ModelMetrics(dfPred,"predResp","target",1,0,"backward", fit$aic)

```

### Multicollinearity 

We will use a value of 5 as our threshold for multicollinearity of our variables[^8]. Here in our backwards selection model we find that `rad` exceeds our pre-established threshold.

```{r, echo=FALSE, results='asis'}
dfVifFit <- setDT(as.data.frame(vif(backFit)), keep.rownames = TRUE)[]
colnames(dfVifFit) <- c("variables", "VIF")
kable(dfVifFit, align = c("l", "c"))
```

[^8]: ["Variance Inflation Factor (VIF)."](http://www.how2stats.net/2011/09/variance-inflation-factor-vif.html)  How2stats:. N.p., n.d. Web. 27 Oct. 2016.

\newpage

## Model 2 - Forwards Selection Method

The simplest data-driven model building approach is called forward selection. In this approach, one adds variables to the model one at a time. At each step, each variable that is not already in the model is tested for inclusion in the model. 

Step function used in this assignment chooses a model by AIC in a Stepwise Algorithm. It continues including variables until the AIC value of <none> variable is the least in the list of variables to choose.

```{r forward selection method, echo=FALSE, results='markup'}
step(lm(target~1, data=wCrimes), direction = "forward", scope = ~zn + I(log(indus)) + I(sqrt(chas)) + I(nox^-1) + I(log(rm)) + I(age^2) + I(dis^-.5) + rad + I(tax^-1) + I(ptratio^2) + I(black^2) + lstat + medv) ##TODO - Review before submission
```

```{r, echo = FALSE, results='asis'}
forwardFit <- fit <- glm(formula = target ~ I(nox^-1) + rad + I(age^2) + medv + I(ptratio^2), data = wCrimes, family = binomial)
stargazer(fit, header = FALSE)
```

```{r forward model metrics, echo=FALSE, results='asis', eval=FALSE}

dfPred <- df[,c("nox","age","dis","rad","tax","ptratio", "black","lstat", "medv")]
predProb <- predict(fit, dfPred, type="response") #Predicted Probability

predResp <- numeric(length(predProb)) #Predicted Class
predResp[which(predProb>=0.59)] <- 1

dfPred <- data.frame(cbind(df[,c("nox","age","dis","rad","tax","ptratio", "black","lstat", "medv")],predProb,predResp))

dfFwdModel <- ModelMetrics(dfPred,"predResp","target",1,0,"forward", fit$aic)


dfBwdModel <- ModelMetrics(dfPred,"predResp","target",1,0,"backward", fit$aic)

```

Here in our forward selection model we find that no variable exceeds our pre-established threshold of 5 for multicollinearity.

```{r, echo=FALSE, results='asis'}
dfVifFit <- setDT(as.data.frame(vif(forwardFit)), keep.rownames = TRUE)[]
colnames(dfVifFit) <- c("variables", "VIF")
kable(dfVifFit, align = c("l", "c"))
```

\newpage

## Model 3 - Subset Selection Method

Using the `leaps` package and the `regsubsets` function we are able to subset our independent variables by looking at the best model for each predictor. The variables as indicated in line `8` of the below table will be further implement into our subset selection model. 

```{r, subset, eval=TRUE, cache=TRUE, results = 'asis', echo=FALSE}
subsetModel <- regsubsets(target ~ ., data = wCrimes, nbest = 1)
summary <- summary(subsetModel)
kable(summary$outmat)
```

```{r, model from subset, eval=TRUE, cache=TRUE, echo=FALSE, results = 'asis'}
subsetModel <- lm(target ~ zn + I(nox^-1) + I(age^2) + rad + I(tax^-1) + I(ptratio^2) + I(black^2) + medv, data = wCrimes) ##TODO - Review before submission
subsetModel <- glm(subsetModel, family = binomial, data = wCrimes)
stargazer(subsetModel, header = FALSE)
```

Here in our subset selection model we find that no variable exceeds our pre-established threshold of 5 for multicollinearity.

```{r, echo=FALSE, results='asis'}
dfVifSubset <- setDT(as.data.frame(vif(subsetModel)), keep.rownames = TRUE)[]
colnames(dfVifSubset) <- c("variables", "VIF")
kable(dfVifSubset, align = c("l", "c"))
```

\newpage 
# Selected Model


\newpage
# Appendix A

## Session Info

```{r, results='asis', echo=FALSE, eval = TRUE}
toLatex(sessionInfo())
```

## Data Dictionary

```{r, eval=TRUE, results='asis', echo=FALSE}
kable(daDict %>% 
      filter(VARIABLE.TYPE == "Predictor Variable") %>%
      mutate(Abbreviation = VARIABLE.NAME, Definition = DEFINITION)  %>%
      dplyr::select(Abbreviation, Definition),
      align = c("c","l"))
```

## R source code

Please see [Homework 3.rmd](https://github.com/ChristopheHunt/DATA-621-Group-1/blob/master/Homework%203/Homework%203.Rmd) on GitHub for source code.   

https://github.com/ChristopheHunt/DATA-621-Group-1/blob/master/Homework%203/Homework%203.Rmd
