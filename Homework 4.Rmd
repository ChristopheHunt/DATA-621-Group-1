---
title: "Homework 4"
author: "Group 1"
date: ''
output:
  pdf_document:
    includes:
      in_header: header.tex
    number_sections: yes
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: '2'
---

```{r programming style guide, echo=FALSE, eval=FALSE}
##Style guide
##USE CAMEL CASING!! https://en.wikipedia.org/wiki/Camel_case, start with lowercase then each word after start with upper case
##upper case for vector, lists, etc. ex. X <- c("yes","no","true","false")
##lower case for scalar/single value, ex. x <- 1
##data frame start with df followed by description, ex. dfEval <- evaluation data frame
##assignments use "<-" not "="
```

```{r library load, include = FALSE}
library(pacman)
p_load(Hmisc, xlsx, xtable, knitr, scales, magrittr, tidyverse, stringr, e1071, corrplot, knitcitations, bibtex, missForest, abc, foreach, doParallel, stargazer, forecast, matrixStats, glmulti, leaps, data.table, highlight, car, pracma, boot)
```

\begin{center}
\bigskip
\bigskip
\bigskip
Prepared for:\\
\medskip
Dr. Nathan Bastian\\
\smallskip
City University of New York, School of Professional Studies - Data 621\\
\bigskip
Prepared by:\\
\medskip
Group 1\\ 
\medskip
Senthil Dhanapal\\ 
\smallskip
Yadu Chittampalli\\
\smallskip
Christophe Hunt\\  

\end{center}

\newpage

# Introduction

Consumers who own a car are often required to purchase car insurance to protect themselves from serious financial repercussions of being involved in a car accident. Insurance Providers must determine the risk of offering insurance coverage to a new customer through accurate statistical models that evaluate the consumers propensity for accidents. Since Insurance Providers are motivated by collecting the maximum amount of revenue from consumers while returning the lowest amount in accident claims, statistical modeling provides Insurance Providers with insight into the consumers behavior and the most appropriate pricing schemes[^1]. 

[^1]: ["Insider Information: How Insurance Companies Measure Risk - Insurance Companies.com."](http://www.insurancecompanies.com/insider-information-how-insurance-companies-measure-risk/) Insurance Companiescom. N.p., n.d. Web. 06 Nov. 2016.

# Statement of the Problem

The purpose of this report is to develop statistical models to make inference into the likelihood of a customer being involved in a car accident and the cost associated of a customer being involved in a car accident. 

# Data Exploration  

## Variables Explained

The variables provided in the `Insurance Training Data Set` are explained below:

```{r data dictionary load, include=FALSE, cache=TRUE}
download.file("https://github.com/ChristopheHunt/DATA-621-Group-1/raw/master/Homework%204/Data%20Dictionary.xlsx?raw=true", destfile = "Data Dictionary HW4 - Group 1.xlsx", mode = 'wb')
daDict <- read.xlsx("Data Dictionary HW4 - Group 1.xlsx", sheetIndex = 1)
file.remove(dir(getwd(), pattern = "Data Dictionary HW4 - Group 1", full.names = TRUE))
```
\footnotesize
```{r data dictionary table, eval=TRUE, results='asis', echo=FALSE, cache=TRUE}
kable(daDict %>% 
      mutate(`Variable Code` = VARIABLE.NAME, Definition = DEFINITION)  %>%
      dplyr::select(`Variable Code`, Definition),
      align = c("c","l"))
```


```{r data prep, echo=FALSE, results='asis', cache=TRUE}
dfTr <- read.csv("https://raw.githubusercontent.com/ChristopheHunt/DATA-621-Group-1/master/Homework%204/insurance_training_data.csv")

removeDollars <- function(column){
             as.numeric(gsub("\\,","",gsub("\\$","", column)))
            }

dfTr$JOB <- as.character(dfTr$JOB)

dfTr$JOB[(dfTr$JOB == "")] <- NA

dfTr <- dfTr %>%
        mutate(INCOME = removeDollars(INCOME),
               HOME_VAL = removeDollars(HOME_VAL),
               BLUEBOOK = removeDollars(BLUEBOOK),
               OLDCLAIM = removeDollars(OLDCLAIM),
               MSTATUS = as.factor(gsub("z_", "", MSTATUS)),
               SEX = as.factor(gsub("z_", "", SEX)),
               EDUCATION = factor((gsub("<", "Less Than ", gsub("z_", "", EDUCATION))), 
                                  levels = c("Less Than High School", "High School", "Bachelors", "Masters", "PhD")),
               JOB = as.factor(gsub("z_", "", JOB)),
               CAR_TYPE = as.factor(gsub("z_", "", CAR_TYPE)),
               URBANICITY = as.factor(gsub("z_", "", URBANICITY)))

```
\normalsize

\newpage

### Nominal Variables

We first look at our nominal variables and their applicable proportions. Interestingly, we see that in this data set only a quarter of the customer records indicate an accident occurred. Also, the majority of consumers in this data set have no kids at home, are married, more than a high school education but less than a PhD, use their car for private purposes, typically own a SUV or minivan, and also live in an urban environment. This provides an interesting insight to the type of customer this data set represents and should be considered when further interpreting our statistical model. Additionally, we should be mindful of any selection biases in this data set as consumers with extremely risky histories are likely to have not been extended insurance coverage. 

```{r nominal variables, echo=FALSE, results='asis', cache=TRUE}
options(xtable.comment = FALSE)

reporttools::tableNominal(dfTr %>% 
                        dplyr::select(TARGET_FLAG, KIDSDRIV, HOMEKIDS, PARENT1, MSTATUS, SEX, 
                                      EDUCATION, JOB, CAR_USE,  CAR_TYPE,RED_CAR, CLM_FREQ, REVOKED, URBANICITY), 
                        cap = "Table of nominal variables", lab = "tab: nominal", print.pval = "fisher",
                        caption.placement = "top", longtable = TRUE, 
                        add.to.row = list(pos = list(0), command = "\\hline \\endhead "))
```

### Continuous and Discrete Variables

We can see that in our continuous and discrete variables there is some additional variability. The median claim amount (`TARGET_AMT`) is 0 which would coincide with only a quarter for records indicating an accident. However, the spread is large since the average payout is only \$1,504.30 but the maximum payout was \$107,586.10. Surprisingly, the median `AGE` is 45 and the average `AGE` is 44.8 years, while we expected a lower average it could be due to simple selection bias in the data set source or the aging US population bringing this average higher [^3]. We also noticed that an `INCOME` of \$0.00 seems unwise because it is unclear how the individual would be able to cover their premium costs without parental support. Finally, we should note that the data set has as `CAR_AGE` of -3, which is impossible and will need to be removed.

There are many missing values for this portion of our data set, we have over 400 values missing for years on the job, income, home value, and car age. Due to these missing values we will need to impute to complete our statistical model. 

[^3]: Ortman, Jennifer M., Victoria A. Velkoff, and Howard Hogan. "An aging nation: the older population in the United States." Washington, DC: US Census Bureau (2014): 25-1140.

```{r continous variables, echo=FALSE, results='asis', cache=TRUE}
reporttools::tableContinuous(dfTr %>% 
                               dplyr::select(TARGET_AMT, TIF, AGE, YOJ, INCOME, HOME_VAL,
                                             TRAVTIME, BLUEBOOK, OLDCLAIM, MVR_PTS, CAR_AGE), 
                             longtable = TRUE)
```

\newpage

## Imputting Missing Values

In order to address the missing values in our variables we used a non-parametric imputation method (Random Forest) using the `missForest` package. The function is particularly useful in that it can handle any type of input data and it will make as few assumptions about the structure of the data as possible.[^2]

[^2]: Stekhoven, Daniel J., and Peter B?hlmann. ["MissForest-non-parametric missing value imputation for mixed-type data." Bioinformatics 28.1 (2012): 112-118](http://bioinformatics.oxfordjournals.org/content/28/1/112.short).

```{r, results='asis', echo = FALSE, include=FALSE, cache=TRUE, eval=FALSE}
registerDoParallel(cl = makeCluster(10), cores = 2)

set.seed(1234)

dfTr$CAR_AGE[(dfTr$CAR_AGE == -3)] <- NA

imputed_data <- dfTr %>% missForest(maxiter = 10, ntree = 100, replace = TRUE, parallelize = 'forests', verbose = TRUE) #Takes approximately 1 hour to process

write.csv(imputed_data$ximp,"imputed_insurance_training_data.csv", row.names = FALSE) #wrote imputed_data to csv file due to processing time taken by missForest
```

```{r, results='asis', echo = FALSE, cache=TRUE, eval=TRUE}
imputedDfTr <- read.csv("https://raw.githubusercontent.com/ChristopheHunt/DATA-621-Group-1/master/Homework%204/imputed_insurance_training_data.csv")

imputedDfTr <- imputedDfTr %>% 
               dplyr::select(-INDEX)

digits = 1

descriptive <- describe(imputedDfTr,
                        descript = "Table 2 : Imputed Descriptive Statistics", 
                        digits = digits)

latex(descriptive, file = '')
```
\newpage

## Exploration of Variables

### Correlation Matrix

If we widen our data set to a matrix to account for the many factors in our evaluation data set we can plot a correlation matrix. We can see some interesting patterns in the data. It is intuitive how both target variables have no correlation with an education level that is less than high school. If the person who wants to buy the vehicle does not have a sufficient level of education, obviously he or she would not have a high-paying job and therefore would not be able to buy the vehicle. It is also intuitive how the target flag variable has negative correlation with marital status. If a married couple owns the vehicle and both the husband and wife share the vehicle, each of them would have to be careful while driving it so as to not get into any accident. It is also intuitive how the target flag variable is negatively correlated with bluebook value. If the bluebook value is high then that means that the car would be very expensive and the owner would have to be careful not to get it into an accident. Otherwise the owner would have to pay lots of money for a new car. 
`PLACEHOLDER TEXT`

```{r correlation matrix, fig.cap= "Correlation Plot of Training Data Set", echo = FALSE, fig.height = 5, fig.width= 7, cache=TRUE, eval=TRUE}

imputedDfTrMx <- imputedDfTr %>%
                   mutate(Yes = 1) %>%
                   spread(EDUCATION, Yes, fill = 0)  %>%
                   mutate(Yes = 1) %>%
                   spread(CAR_TYPE, Yes, fill = 0) %>%
                   mutate(Yes = 1) %>%
                   mutate(SEX = ifelse(SEX == "F", "Female", "Male")) %>%
                   spread(SEX, Yes, fill = 0) %>%
                   mutate(Yes = 1) %>%
                   mutate(CAR_USE = ifelse(CAR_USE == "Commercial", "Commercial Car Use", "Private Car Use")) %>%
                   spread(CAR_USE, Yes, fill = 0) %>%
                   mutate(Yes = 1) %>%
                   spread(URBANICITY, Yes, fill = 0 ) %>%
                   mutate(PARENT1 = ifelse(PARENT1 == "No", 0, 1),
                          MSTATUS = ifelse(MSTATUS == "No", 0, 1),
                          RED_CAR = ifelse(RED_CAR == "no", 0, 1),
                          REVOKED = ifelse(REVOKED == "No", 0, 1)) %>%
                  mutate(Yes = 1) %>%
                  spread(JOB, Yes, fill = 0)  

imputedDfTrMx <- as.matrix(imputedDfTrMx)

corMx <- cor(imputedDfTrMx , use = "everything",  method = c("pearson"))
corrplot(corMx, order = "hclust", addrect = 10, method = "square", tl.col = "black", tl.cex = .50, na.label = " ")
```

\newpage

# Data Transformation

## Outliers Treatment

We chose winsorizing as the method to address outliers. Instead of trimming values, winsorizing uses the interquantile range to replace values that are above or below the interquantile range multiplied by a factor. Those values above or below the range multiplied by the factor are then replaced with max and min value of the interquantile range. Using the factor 2.2 for winsorizing outliers is a method developed my Hoaglin and Iglewicz and published Journal of American Statistical Association in 1987[^7].  

[^7]:Hoaglin, D. C., and Iglewicz, B. (1987), Fine tuning some resistant rules for outlier labeling, Journal of American Statistical Association, 82, 1147-1149.

The below table is the summary results of the winsorizing of the data. 


```{r winsorize dataframe,  echo=FALSE, results='asis',  cache=TRUE, eval=TRUE}
library(forecast)

winsorize <- function(x, multiple=2.2)
{
  q <- quantile(x)
  iqr <- IQR(x)
  iqrAdjusted <- iqr*multiple

  rangeLow <- q['25%']-iqrAdjusted
  rangeHigh <- q['75%']+iqrAdjusted

  x[x<rangeLow] <- min(x[x>rangeLow])
  x[x>rangeHigh] <- max(x[x<rangeHigh])

  return(x)  

}

wimputedDfTr <- cbind(sapply(c('INCOME', 'HOME_VAL', 'OLDCLAIM', 'BLUEBOOK'), function(x) winsorize(imputedDfTr[,x], multiple=2.2)), subset(imputedDfTr, select = -c(INCOME, HOME_VAL, OLDCLAIM, BLUEBOOK)))

stargazer(wimputedDfTr, header = FALSE)
```

\newpage

## BoxCox Transformations 

The Box-Cox transformations were done only on three of the input variables - income, house value, and the total number of claims during the past 5 years. These transformations were done based on the residual plots. In the residual plots, these three variables showed a great deal of non-constant variance because the plots were funnel-shaped. 

```{r, echo=FALSE, results = 'asis', cache=TRUE}
library(car)
fit <- lm(TARGET_AMT ~ KIDSDRIV + AGE + HOMEKIDS + YOJ + INCOME + PARENT1 + HOME_VAL + MSTATUS + SEX + EDUCATION + JOB + TRAVTIME + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + RED_CAR + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + URBANICITY, data=wimputedDfTr)

ncvTest(fit, ~ KIDSDRIV + AGE + HOMEKIDS + YOJ + INCOME + PARENT1 + HOME_VAL + MSTATUS + SEX + EDUCATION + JOB + TRAVTIME + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + RED_CAR + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + URBANICITY, data=wimputedDfTr)
```

```{r, echo=FALSE,cache=TRUE, message = FALSE}
residualPlots(fit)
```

\newpage

Using the `BoxCox.lambda` function from the `forecast` package we are able to determine our necessary transformations to our independent variables. 

```{r, echo=FALSE, cache=TRUE}
l1 <- BoxCox.lambda(as.numeric(wimputedDfTr$INCOME))
l2 <- BoxCox.lambda(as.numeric(wimputedDfTr$HOME_VAL))
l3 <- BoxCox.lambda(as.numeric(wimputedDfTr$TRAVTIME))

lamdas <- c(l1, l2, l3)
Variables <- c("INCOME", "HOME_VAL", "TRAVTIME")
dfBoxCox <- as.data.frame(cbind(lamdas, Variables))

colnames(dfBoxCox) <- c("$\\lambda$", "Variables")

kable(dfBoxCox, align = c("c", "c"))
```

Utilizing the below table of common transformations based on the lambda value of the BoxCox we further transform our independent variables.

\centering

Common Box-Cox Transformations[^8] [^4]

\setlength{\tabcolsep}{12pt}

\begin{tabular}{ c c }
\hline
$\lambda$ & Y' \\ \hline
-2 &	$Y^{-2}~=~\frac{1}{Y^{2}}$ \\
-1 &	$Y^{-1}~=~\frac{1}{Y^{1}}$ \\
-0.5 &	$Y^{-0.5}~=~\frac{1}{\sqrt{(Y)}}$ \\
0	& $\log(Y)$ \\
.25  & $\sqrt[4]{Y}$\\  
0.5	& $Y^{0.5}~=~\sqrt{(Y)}$ \\
1	& $Y^{1}~=~Y$ \\
2	& $Y^{2}$ \\

\end{tabular}

\justifying

Lambda values were truncated to the nearest tenth that match a common transformation as per the below table.

\centering

\begin{tabular}{ c c }
\hline
variable & variable transformation \\ \hline
INCOME & $\sqrt[4]{INCOME}$ \\
HOME VAL & $\sqrt(HOME~VAL)$ \\
TRAVTIME & $\sqrt{TRAVTIME}$ 
\end{tabular}

\justifying

\setlength{\tabcolsep}{6pt}

[^4]: Osborne, Jason W. "Improving your data transformations: Applying the Box-Cox transformation." Practical Assessment, Research & Evaluation 15.12 (2010): 1-9.

[^8]: [By Understanding Both the Concept of Transformation and the Box-Cox Method, Practitioners Will Be Better Prepared to Work with Non-normal Data.](https://www.isixsigma.com/tools-templates/normality/making-data-normal-using-box-cox-power-transformation/) . "Making Data Normal Using Box-Cox Power Transformation." ISixSigma. N.p., n.d. Web. 29 Oct. 2016.

\newpage

# Models Built

```{r model metrics, echo=FALSE,  message=FALSE,cache=TRUE}
#Supporting Functions

library(boot)

TP <- function(df, predResp, resp, p, n)
{
  return(length(df[which(df[predResp]==p & df[resp]==p),1]))
}

TN <- function(df, predResp, resp, p, n)
{
  return(length(df[which(df[predResp]==n & df[resp]==n),1]))
}

FP <- function(df, predResp, resp, p, n)
{
  return(length(df[which(df[predResp]==p & df[resp]==n),1]))
}

FN <- function(df, predResp, resp, p, n)
{
  return(length(df[which(df[predResp]==n & df[resp]==p),1]))
}


Accuracy <- function(df, predResp, resp, p, n)
{
  tp <- TP(df,predResp,resp,p,n)
  tn <- TN(df,predResp,resp,p,n)
  fp <- FP(df,predResp,resp,p,n)
  fn <- FN(df,predResp,resp,p,n)
  
  return ((tp + tn)/(tp + fp + tn + fn))
  
}

ClassificationErrorRate <- function(df, predResp, resp, p, n)
{
  tp <- TP(df,predResp,resp,p,n)
  tn <- TN(df,predResp,resp,p,n)
  fp <- FP(df,predResp,resp,p,n)
  fn <- FN(df,predResp,resp,p,n)
  
  return ((fp + fn)/(tp + fp + tn + fn))
  
}

Precision <- function(df, predResp, resp, p, n)
{
  tp <- TP(df,predResp,resp,p,n)
  fp <- FP(df,predResp,resp,p,n)
  
  return (tp/(tp + fp))
  
}

Sensitivity <- function(df, predResp, resp, p, n)
{
  tp <- TP(df,predResp,resp,p,n)
  fn <- FN(df,predResp,resp,p,n)
  
  return (tp/(tp + fn))
  
}

Specificity <- function(df, predResp, resp, p, n)
{
  tn <- TN(df,predResp,resp,p,n)
  fp <- FP(df,predResp,resp,p,n)
  
  return (tn/(tn + fp))
  
}

F1Score <- function(df, predResp, resp, p, n)
{
  precision <- Precision(df,predResp,resp,p,n)
  sensitivity <- Sensitivity(df,predResp,resp,p,n)
  
  return ((2 * precision * sensitivity)/(precision + sensitivity))
  
}


ROC <- function(df, respProb, resp, p, n, thresholds, plotTitle)
{
  x <- thresholds
  dfForPlot <- data.frame(threshold=integer(), fpr=double(),tpr=double(), auc=double(), dist=double())
  for(i in x)
  {
    plotCoord <- ROC.Coordinates(df,resp, respProb, p, n, i)
    #print(plotCoord)
    dfForPlot <-  data.frame(rbind(dfForPlot,data.frame(plotCoord)))
  }
  plot(x=dfForPlot$fpr,y=dfForPlot$tpr,lwd=2, type="l", xlab="FPR", ylab="TPR", main=plotTitle)
  lines(x=c(0, 1), y=c(0, 1), col="black", lwd=1)
  return (dfForPlot)
  
}


ROC.Coordinates <- function(df, resp, respProb, p, n, threshold)
{
  pred <- rep(n,length(df[,1]))
  dfNew <- df
  pred[which(df[respProb]>=threshold)]  <- p
  dfNew$pred <- as.factor(pred)
  
  sensitivity <- Sensitivity(df = dfNew, predResp = "pred",resp = resp,p = p,n = n)
  specificity <- Specificity(df = dfNew, predResp = "pred",resp = resp,p = p,n = n)
  
  tpr <- sensitivity
  fpr <- 1 - specificity
  auc <- (sensitivity + specificity)/2
  dist <- sqrt((1-tpr)^2 + (fpr)^2)
  
  return (data.frame(threshold,fpr,tpr,auc,dist))
  
}

calculateAUC <- function(curveInfo)
{
  tpr1 <- 0
  fpr1 <- 0
  auc <- 0
  curveInfo <- curveInfo[order(-curveInfo$threshold),]
  for(i in 1:nrow(curveInfo))
  {
    tpr2 <- curveInfo[i,"tpr"]
    fpr2 <- curveInfo[i,"fpr"]
    
    auc <- auc + ((tpr1 + tpr2)/2)*(fpr2-fpr1)
    
    
    tpr1 <- tpr2
    fpr1 <- fpr2
  }
  return(auc)
}


ConfusionDataFrame <- function(tp,fp,tn,fn)
{
  cm <- data.frame( rbind(tp,fp), rbind(fn, tn))
  colnames(cm) <- c("Act-Pos","Act-Neg")
  rownames(cm) <- c("Pred-Pos","Pred-Neg")
  return(kable(cm))
}

ModelMetrics.LogReg <- function(df, predResp, resp, p, n, modelName, fit)
{
  acc <- Accuracy(df,predResp,resp,p,n)
  cer <- ClassificationErrorRate(df,predResp,resp,p,n)
  prec <- Precision(df,predResp,resp,p,n)
  sens <- Sensitivity(df,predResp,resp,p,n)
  spec <- Specificity(df,predResp,resp,p,n)
  f1score <- F1Score(df,predResp,resp,p,n)
  roc <- ROC(df,"predProb",resp,p,n, seq(0,p,.01),modelName)
  
  auc <- calculateAUC(roc)
  
  bestThreshold <- roc[which(roc$auc == max(roc$auc)),][1,][,1]

  
  tp <- TP(df,predResp,resp,p,n)
  tn <- TN(df,predResp,resp,p,n)
  fp <- FP(df,predResp,resp,p,n)
  fn <- FN(df,predResp,resp,p,n)

  print(ConfusionDataFrame(tp,fp,tn,fn))
  
  cv.err <- boot::cv.glm(df,fit,K=10)$delta[1]  
  
  M <- format(c(acc,cer,prec,sens,spec,f1score,auc,bestThreshold,fit$aic,cv.err), digits = 3)
  dfReturn <- data.frame("metric" = M)
  r.n <- c("accuracy", "classif.error","precision","sensitivity","specificity","f1score","auc","best.threshold", "aic","CVError")
  rownames(dfReturn) <- r.n
  colnames(dfReturn) <- modelName
  
  
  return(dfReturn)
}


ModelMetrics.LinReg <- function(modelName, fit)
{
  s.fit <- summary(fit)
  adj.r.sqrd <- s.fit$adj.r.squared
  r.sqrd <- s.fit$r.squared
  f.stats <- s.fit$fstatistic[1]
  
  mse <- sum(fit$residuals^2)/s.fit$df[2]
  rmse <-sqrt(mse)
  mae <- mean(abs(fit$residuals))

  p <- par(mfrow=c(2,2))
  plot(fit)
  par(p)
  

  M <- round(c(adj.r.sqrd,mse,rmse,mae,r.sqrd,f.stats), digits = 3)
  df <- data.frame("metric" = M)
  r.n <- c("Adjusted R-Sqrd", "MSE", "RMSE","MAE","R-Sqrd","f.stats")
  rownames(df) <- r.n
  colnames(df) <- modelName
  return(df)
}


```

## Logistic Model 1 - Backwards Selection Method

In the backward step selection model  The resulting AIC was . 

```{r backwards model selection, echo=FALSE, results='asis',  cache=TRUE, eval=TRUE}
fullModel <- lm(TARGET_FLAG ~ KIDSDRIV + AGE + HOMEKIDS + YOJ + I(INCOME^(1/4)) + PARENT1 + I(sqrt(HOME_VAL)) + MSTATUS + SEX + EDUCATION + JOB + I(sqrt(TRAVTIME)) + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + RED_CAR + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + URBANICITY, data = wimputedDfTr)
backFit <- fit <- glm(step(fullModel, direction = "backward", trace = F))

stargazer(fit, header = FALSE, no.space = TRUE, style = "qje", font.size = "normalsize", single.row = TRUE, intercept.bottom = FALSE)
```

\newpage

### Model Metrics for Backwards Selection

We first use an established threshold of .50 to determine our best possible threshold. 

```{r backwards model metrics, echo=FALSE, results='asis', eval=TRUE, cache=TRUE}

dfPred <- wimputedDfTr[,c("KIDSDRIV", "INCOME", "PARENT1", "HOME_VAL", "MSTATUS", "EDUCATION", "JOB", "TRAVTIME", "CAR_USE", "BLUEBOOK", "TIF", "CAR_TYPE", "OLDCLAIM", "CLM_FREQ", "REVOKED", "MVR_PTS", "URBANICITY")]

predProb <- predict(fit, dfPred, type = "response") #Predicted Probability
predResp <- numeric(length(predProb)) #Predicted Class
predResp[which(predProb>=0.50)] <- 1

dfPred <- data.frame(cbind(wimputedDfTr[,c( "KIDSDRIV", "INCOME", "PARENT1", "HOME_VAL", "MSTATUS", "EDUCATION", "JOB", "TRAVTIME", "CAR_USE", "BLUEBOOK", "TIF", "CAR_TYPE", "OLDCLAIM", "CLM_FREQ", "REVOKED", "MVR_PTS", "URBANICITY", "TARGET_FLAG")], predProb, predResp))

dfBackModel <- ModelMetrics.LogReg(df = dfPred, predResp = "predResp", resp = "TARGET_FLAG",
                                  p = 1, n = 0,
                                  modelName = "Model Metrics for Backwards Selection", fit = fit)

kable(dfBackModel, align = c("l","c"))
```


Our previous results indicate that .320 would be the best threshold for this model so we re-run our metrics using this threshold.

\newpage

#### Model Metrics for Backwards Selection with best threshold

Model Metrics using best threshold of .320. 

```{r backwards model metrics best threshold, echo=FALSE, results='asis', eval=TRUE, cache=TRUE}

dfPred <- wimputedDfTr[,c("KIDSDRIV", "INCOME", "PARENT1", "HOME_VAL", "MSTATUS", "EDUCATION", "JOB", "TRAVTIME", "CAR_USE", "BLUEBOOK", "TIF", "CAR_TYPE", "OLDCLAIM", "CLM_FREQ", "REVOKED", "MVR_PTS", "URBANICITY")]

predProb <- predict(fit, dfPred, type = "response") #Predicted Probability

predResp <- numeric(length(predProb)) #Predicted Class

predResp[which(predProb>=0.32)] <- 1

dfPred <- data.frame(cbind(wimputedDfTr[,c( "KIDSDRIV", "INCOME", "PARENT1", "HOME_VAL", "MSTATUS", "EDUCATION", "JOB", "TRAVTIME", "CAR_USE", "BLUEBOOK", "TIF", "CAR_TYPE", "OLDCLAIM", "CLM_FREQ", "REVOKED", "MVR_PTS", "URBANICITY", "TARGET_FLAG")], predProb, predResp))

dfBackModel <- ModelMetrics.LogReg(df = dfPred, predResp = "predResp", resp = "TARGET_FLAG",
                                  p = 1, n = 0,
                                  modelName = "Model Metrics for Backwards Selection with best threshold", fit = fit)

kable(dfBackModel, align = c("l","c"))

```

\newpage

### Multicollinearity for Backwards Selection

We square `GVIF^(1/(2*Df))` in order to use the VIF threshold of 5 for multicollinearity. Here in our subset selection model we find that no variable exceeds our pre-established threshold of 5 for multicollinearity.

```{r, echo=FALSE, results='asis',  cache=TRUE}
dfVifFit <- setDT(as.data.frame(car::vif(backFit)), keep.rownames = TRUE)[]
dfVifFit$Adjusted_GVIF <- (dfVifFit$`GVIF^(1/(2*Df))`^2)
kable(dfVifFit, align = c("l", "c", "c", "c", "c"))
```

\newpage

## Logistic Regression 2 - Forwards Selection Method

```{r forward selection method, echo=FALSE, results='asis', cache=TRUE}
fullModel <- lm(TARGET_FLAG ~ KIDSDRIV + AGE + HOMEKIDS + YOJ + I(INCOME^(1/4)) + PARENT1 + I(sqrt(HOME_VAL)) + MSTATUS + SEX + EDUCATION + JOB + I(sqrt(TRAVTIME)) + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + RED_CAR + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + URBANICITY, data = wimputedDfTr)
forwardFit <- fit <- glm(step(fullModel, direction = "forward", trace = F))
stargazer(fit, header = FALSE, no.space = TRUE, style = "qje", font.size = "normalsize", single.row = TRUE, intercept.bottom = FALSE)
```

\newpage

### Model Metrics for Forwards Selection

We first use an established threshold of .50 to determine our best possible threshold. 

```{r forwards model metrics, echo=FALSE, results='asis', eval=TRUE, cache=TRUE}
dfPred <- wimputedDfTr[,c("KIDSDRIV", "AGE", "HOMEKIDS", "YOJ", "INCOME", "PARENT1", "HOME_VAL", "MSTATUS", "SEX", "EDUCATION", "JOB", "TRAVTIME", "CAR_USE", "BLUEBOOK", "TIF", "RED_CAR", "CAR_TYPE", "OLDCLAIM", "CLM_FREQ", "REVOKED", "MVR_PTS", "CAR_AGE", "URBANICITY")]

predProb <- predict(fit, dfPred, type = "response") #Predicted Probability

predResp <- numeric(length(predProb)) #Predicted Class

predResp[which(predProb>=0.50)] <- 1

dfPred <- data.frame(cbind(wimputedDfTr[,c("KIDSDRIV", "AGE", "HOMEKIDS", "YOJ", "INCOME", "PARENT1", "HOME_VAL", "MSTATUS", "SEX", "EDUCATION", "JOB", "TRAVTIME", "CAR_USE", "BLUEBOOK", "TIF", "RED_CAR", "CAR_TYPE", "OLDCLAIM", "CLM_FREQ", "REVOKED", "MVR_PTS", "CAR_AGE", "URBANICITY", "TARGET_FLAG")], predProb, predResp))

dfFwdModel <- ModelMetrics.LogReg(df = dfPred, predResp = "predResp", resp = "TARGET_FLAG",
                                  p = 1, n = 0,
                                  modelName = "Model Metrics for Forwards Selection", fit = fit)


kable(dfFwdModel, align = c("l","c"))
```

Our previous results indicate that .310 would be the best threshold for this model so we re-run our metrics using this threshold.

\newpage

#### Model Metrics for Forwards Selection with best threshold

Model Metrics using best threshold of .310. 

```{r forwards model metrics with best threshold, echo=FALSE, results='asis', eval=TRUE, cache=TRUE}
dfPred <- wimputedDfTr[,c("KIDSDRIV", "AGE", "HOMEKIDS", "YOJ", "INCOME", "PARENT1", "HOME_VAL", "MSTATUS", "SEX", "EDUCATION", "JOB", "TRAVTIME", "CAR_USE", "BLUEBOOK", "TIF", "RED_CAR", "CAR_TYPE", "OLDCLAIM", "CLM_FREQ", "REVOKED", "MVR_PTS", "CAR_AGE", "URBANICITY")]

predProb <- predict(fit, dfPred, type = "response") #Predicted Probability

predResp <- numeric(length(predProb)) #Predicted Class

predResp[which(predProb>=0.310)] <- 1

dfPred <- data.frame(cbind(wimputedDfTr[,c("KIDSDRIV", "AGE", "HOMEKIDS", "YOJ", "INCOME", "PARENT1", "HOME_VAL", "MSTATUS", "SEX", "EDUCATION", "JOB", "TRAVTIME", "CAR_USE", "BLUEBOOK", "TIF", "RED_CAR", "CAR_TYPE", "OLDCLAIM", "CLM_FREQ", "REVOKED", "MVR_PTS", "CAR_AGE", "URBANICITY", "TARGET_FLAG")], predProb, predResp))

dfFwdModel <- ModelMetrics.LogReg(df = dfPred, predResp = "predResp", resp = "TARGET_FLAG",
                                  p = 1, n = 0,
                                  modelName = "Model Metrics for Forwards Selection", fit = fit)

kable(dfFwdModel, align = c("l","c"))
```
\newpage

### Multicollinearity for Forwards Selection

We square `GVIF^(1/(2*Df))` in order to use the VIF threshold of 5 for multicollinearity. Here in our subset selection model we find that no variable exceeds our pre-established threshold of 5 for multicollinearity.

```{r, echo=FALSE, results='asis',  cache=TRUE}
dfVifFit <- setDT(as.data.frame(car::vif(forwardFit)), keep.rownames = TRUE)[]
dfVifFit$Adjusted_GVIF <- (dfVifFit$`GVIF^(1/(2*Df))`^2)
kable(dfVifFit, align = c("l", "c", "c", "c", "c"))
```

\newpage

## Logistic Regression 3 - Subset Selection Method

### Subset Variable Selection

Using the `leaps` package and the `regsubsets` function we are able to subset our independent variables by looking at the best model for each predictor. 

```{r, subset, eval=TRUE, cache=TRUE, results = 'asis', echo=FALSE}
subsetModel <- regsubsets(TARGET_FLAG ~ KIDSDRIV + AGE + HOMEKIDS + YOJ + I(INCOME^(1/4)) + PARENT1 + I(sqrt(HOME_VAL)) + MSTATUS + SEX + EDUCATION + JOB + I(sqrt(TRAVTIME)) + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + RED_CAR + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + URBANICITY, data = wimputedDfTr, nbest = 1)
summary <- summary(subsetModel)
kable(t(summary$outmat))
```

\newpage

### Subset Model

The variables as indicated in column `8` of the previous table will be further implement into our subset selection model in the following table. We don't see as strong of a relationship in our independent variables to the dependent variable in this model as our previous model. 

```{r, model from subset, eval=TRUE, cache=TRUE, echo=FALSE, results = 'asis'}
subsetModelFormula <- lm(TARGET_FLAG ~ KIDSDRIV + I(INCOME^(1/4)) + MSTATUS + JOB + CAR_USE + REVOKED + MVR_PTS + URBANICITY, data = wimputedDfTr) ##TODO - Review before submission
subsetModel <- fit <- glm(subsetModelFormula, family = binomial, data = wimputedDfTr)
stargazer(fit, header = FALSE, no.space = TRUE, style = "qje", font.size = "normalsize", single.row = TRUE, intercept.bottom = FALSE)
```

\newpage

### Model Metrics for Subset Selection

We first use an established threshold of .50 to determine our best possible threshold. 

```{r subset model metrics, echo=FALSE, results='asis', eval=TRUE,  cache=TRUE, error=TRUE}
dfPred <- wimputedDfTr[,c("KIDSDRIV", "INCOME", "MSTATUS", "JOB", "CAR_USE", "REVOKED", "MVR_PTS", "URBANICITY")]
predProb <- predict(fit, dfPred, type="response") #Predicted Probability

predResp <- numeric(length(predProb)) #Predicted Class
predResp[which(predProb>=0.50)] <- 1

dfPred <- data.frame(cbind(wimputedDfTr[,c("KIDSDRIV", "INCOME", "MSTATUS", "JOB", "CAR_USE", "REVOKED", "MVR_PTS", "URBANICITY", "TARGET_FLAG")], predProb, predResp))

dfSubModel <- ModelMetrics.LogReg(df = dfPred, predResp = "predResp", resp = "TARGET_FLAG",
                                  p = 1, n = 0,
                                  modelName = "Model Metrics for Subset Selection", fit = fit)

kable(dfSubModel, align = c("l","c"))
```

\newpage

### Multicollinearity for Subset Selection

We square `GVIF^(1/(2*Df))` in order to use the VIF threshold of 5 for multicollinearity. Here in our subset selection model we find that no variable exceeds our pre-established threshold of 5 for multicollinearity. 

```{r, echo=FALSE, results='asis',  cache=TRUE, eval=TRUE}
dfVifFit <- setDT(as.data.frame(car::vif(subsetModel)), keep.rownames = TRUE)[]
dfVifFit$Adjusted_GVIF <- (dfVifFit$`GVIF^(1/(2*Df))`^2)
kable(dfVifFit, align = c("l", "c", "c", "c", "c"))
```

\newpage

## Linear Regression 1 - Backwards Selection

```{r, echo = FALSE, results = 'asis', cache=TRUE, eval=TRUE}
fullModel <- lm(TARGET_AMT ~ KIDSDRIV + AGE + HOMEKIDS + YOJ + I(INCOME^(1/4)) + PARENT1 + I(sqrt(HOME_VAL)) + MSTATUS + SEX + EDUCATION + JOB + I(sqrt(TRAVTIME)) + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + RED_CAR + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + URBANICITY, data = wimputedDfTr)

bkFitStep <- step(fullModel, direction = "backward", trace = F)

print(formula(bkFitStep))

backFitMLR <-  lm(formula(bkFitStep),data = wimputedDfTr)

stargazer(backFitMLR, header = FALSE, no.space = TRUE, style = "qje", font.size = "normalsize", single.row = TRUE, intercept.bottom = FALSE)
```
\newpage

### Linear Regression 1 - Backwards Selection Model Metrics

```{r, echo = FALSE, results = 'asis', cache=TRUE, eval=TRUE}
bkMLRMetrics <- ModelMetrics.LinReg("BackMLR",backFitMLR)
```

\newpage

## Linear Regression 2 - Forwards Selection 

```{r, echo = FALSE, results = 'asis', cache=TRUE, eval=TRUE}
fullModel <- lm(TARGET_AMT~1,data=wimputedDfTr)

fwdFitstep <- step(fullModel,direction = "forward",
      scope=~KIDSDRIV + AGE + HOMEKIDS + YOJ + I(INCOME^(1/4)) + PARENT1 + I(sqrt(HOME_VAL)) 
     + MSTATUS + SEX + EDUCATION + JOB + I(sqrt(TRAVTIME)) + CAR_USE + BLUEBOOK + TIF 
     + CAR_TYPE + RED_CAR + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE 
     + URBANICITY,trace = F)

print(formula(fwdFitstep))

fwdFitMLR <- lm(formula(fwdFitstep), data = wimputedDfTr)

stargazer(fwdFitMLR, header = FALSE, no.space = TRUE, style = "qje", font.size = "normalsize", single.row = TRUE, intercept.bottom = FALSE)
```

### Linear Regression 2 - Forwards Selection Model Metrics

```{r, echo = FALSE, results = 'asis', cache=TRUE, eval=TRUE}
fwdMLRMetrics <- ModelMetrics.LinReg("FwdMLR",fwdFitMLR)
```

#Selected Models 

## Logistic Regression Selected

```{r}
cbind(dfFwdModel, dfSubModel, dfBackModel)
```


