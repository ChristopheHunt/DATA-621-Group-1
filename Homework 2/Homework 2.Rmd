---
title: "Homework 2"
author: "Group 1"
date: ''
output:
  pdf_document:
    includes:
      in_header: header.tex
    number_sections: yes
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: '2'
---

\begin{center}
\bigskip
\bigskip
\bigskip
Prepared for:\\
\medskip
Dr. Nathan Bastian\\
\smallskip
City University of New York, School of Professional Studies - Data 621\\
\bigskip
Prepared by:\\
\medskip
Group 1\\ 
\medskip
Senthil Dhanapal\\ 
\smallskip
Yadu Chittampalli\\
\smallskip
Christophe Hunt\\  

\end{center}

```{r, include = FALSE}
library(pacman)
p_load(xtable, knitr, pander, xtable, scales, tidyverse, formatR, caret)
tidy.opts=list(width.cutoff=60)
```


# Data Source 

The data is a set of actual classes and predicted classes as provided by Dr. Nathan Bastian for this exercise. We uploaded the data to our public GitHub repository for ease of access. 

```{r load in data, echo=FALSE}
classification <- read.csv("https://raw.githubusercontent.com/ChristopheHunt/DATA-621-Group-1/master/Homework%202/classification-output-data.csv")
```

# Data Explained and Confusion Matrix

We will be using the following columns from the data source: 

* class: the actual class for the observation
* scored.class: the predicted class for the observation (based on a threshold of 0.5)
* scored.probability: the predicted probability of success for the observation

```{r subset data, echo=FALSE, results = 'asis', cache=TRUE}
scores <- subset(classification, select = c("class", "scored.class", "scored.probability"))
```

Below is the raw confusion matrix for our scored data set

```{r, echo=FALSE, results = 'asis', cache=TRUE}
m <- table("Predicted" = scores$class, "Actual" = scores$scored.class)
colnames(m) <- c('Predicted Failure', "Predicted Success")
rownames(m) <- c('Actual Failure', 'Actual Success')
kable(m)
```

Here the rows represent the actual classes and the columns represent the predicted classes.

# Function for Accuracy of Predictions 

We developed a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the accuracy of the predictions. 

Accuracy is determined by the below equation: 

$$
\begin{aligned}
Accuracy~=~\frac{True~Positives~+~True~Negatives}{True~Positives~+~False~Positives~+~True~Negatives~+~False~Negatives}\end{aligned}
$$

```{r, echo = FALSE, results='asis'}
accuracy <- function(df, actual, prediction){
  
  library(scales)
  library(dplyr)
  library(tidyr)
  
  if (sum(colnames(df) %in% c(actual, prediction)) != 2){
    return("One or more columns were not found, please verify selections")
  }
  truefalse <- as.data.frame(table(df[actual] == df[prediction])) %>% 
               spread('Var1', 'Freq')
  
  accuracy <- unlist(truefalse[2]/nrow(df))
  
  return(as.numeric(accuracy))
}

accuracy(scores,"class", "scored.class" )
```

# Function for Classification Error Rate of Predictions 

We developed a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the classification error rate of the predictions. It also verifies that the accuracy and an error rate sums to one. 

Classification of Error Rate is determined by the below formula:
$$
\begin{aligned}
Classification~Error~Rate~=~\frac{False~Positives~+~False~Negatives}{True~Positives~+~False~Positives~+~True~Negatives~+~False~Negatives}
\end{aligned}
$$

```{r, echo=FALSE}
errorrate <- function(df, actual, prediction){
  
  truefalse <- data.frame(table(df[actual] == df[prediction])) %>% 
                          spread('Var1', 'Freq')
  
  error <- unlist(truefalse[1]/nrow(df))
  
  if (error + accuracy(df, actual, prediction) != 1){return("Accuracy and Error Rate does not sum to 1")}
  
  return(as.numeric(error))
}

```

# Function for Precisions of Predictions

Write a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the precision of the predictions. - Senthil

Precision is determined by the below formula:
$$
\begin{aligned}
Precision~=~\frac{True~Positives}{True~Positives~+~False~Positives}
\end{aligned}
$$


# Function for Sensitivity of Predictions

Write a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the sensitivity of the predictions. Sensitivity is also known as recall. - Senthil

Sensitivity is determined by the below formula:
$$
\begin{aligned}
Sensitivity~=~\frac{True~Positives}{True~Positives~+~False~Negatives}
\end{aligned}
$$


# Function for Specificity of Predictions 

```{r, echo=FALSE}
Specificity <- function(df, actual, prediction){
  
  if(sum(colnames(df) %in% c(actual, prediction)) != 2){ 
    return("One or more columns were not found, please verify selections")
  }
  m <- as.data.frame(table(df[[actual]], df[[prediction]]))
  true_negative  <- m$Freq[m$Var1 == 0 & m$Var2 == 0]
  false_positive <- m$Freq[m$Var1 == 0 & m$Var2 == 1]
  return(true_negative/(true_negative + false_positive))
  }

```


# F1 Score of Predictions 

We developed a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the F1 score of the predictions. 

```{r}
f1_score <- function(df, actual, prediction){
  require(scales)
  if (sum(colnames(df) %in% c(actual, prediction)) != 2) {
    return("One or more columns were not found, please verify selections")
  }
  
  m <- as.data.frame(table(df[[actual]], df[[prediction]])) 
  
  true_positive  <- as.numeric(m$Freq[m$Var1 == 1 & m$Var2 == 1])
  false_positive <- as.numeric(m$Freq[m$Var1 == 0 & m$Var2 == 1])
  false_negative <- as.numeric(m$Freq[m$Var1 == 1 & m$Var2 == 0])
  
  precision <- true_positive/(true_positive + false_positive)
  sensitivity <- true_positive/(true_positive + false_negative)
  f1_score <- ((2 * precision * sensitivity) / (precision + sensitivity))
  return(f1_score)
  }



```


# Bounds of F1 Score of Predictions 

Before we move on, let's consider a question that was asked: What are the bounds on the F1 score? Show
that the F1 score will always be between 0 and 1. (Hint: If 0 < ???? < 1 and 0 < ???? < 1 then ????????- Christophe

# Function for ROC curve 

Write a function that generates an ROC curve from a data set with a true classification column (class in our
example) and a probability column (scored.probability in our example). Your function should return a list
that includes the plot of the ROC curve and a vector that contains the calculated area under the curve
(AUC). Note that I recommend using a sequence of thresholds ranging from 0 to 1 at 0.01 intervals. - Senthil

# R Functions created and classification output

##Accuracy of Predictions 
```{r, tidy=TRUE, cache=TRUE, results='asis'}
paste0("Accuracy of Predictions = ", percent(accuracy(df = scores, actual = 'class', prediction = 'scored.class')))
```

##Classification Error Rate of Predictions 
```{r, tidy=TRUE, cache=TRUE, results='asis'}
paste0("Error Rate of Predictions = ", percent(errorrate(scores, "class", "scored.class")))
```

##Precisions of Predictions
```{r, tidy=TRUE, cache=TRUE, results='asis'}

```

##Sensitivity of Predictions
```{r, tidy=TRUE, cache=TRUE, results='asis'}

```

##Specificity of Predictions

```{r, tidy=TRUE, cache=TRUE, results='asis'}
paste0("Specificity of Predictions = ", percent(Specificity(df = scores, actual = 'class', prediction = 'scored.class')))
```


##F1 Score of Predictions

```{r, tidy=TRUE, cache=TRUE, results='asis'}
paste0("The F1 Score = ", f1_score(df = scores, actual = 'class', prediction = 'scored.class'))
```

# Investigation of `caret` package. 

In particular, consider the functions confusionMatrix, sensitivity, and specificity. Apply the functions to the data set. How do the results compare with your own functions? 

```{r, echo = FALSE, results='asis'}
library(caret)
as.matrix(confusionMatrix(scores[,1], scores[,2]))
```

Here, the accuracy is rounded up to 4 decimal places. The confusion matrix has rows that represent the predicted classes and columns that represent the actual classes. 

```{r, echo = FALSE, results='asis'}
sensitivity(table(scores[,1], scores[,2]))
```


```{r, echo = FALSE, results='asis'}
specificity(table(scores[,1], scores[,2]))
```

# Investigation of the `pROC` package. 

Use it to generate an ROC curve for the data set. How do the results compare with your own functions? - Senthil 

# Appendix A

## Session Info

```{r, results='asis', echo=FALSE, eval = TRUE}

toLatex(sessionInfo())
```

## Data Summary

```{r, results='asis', echo=FALSE, cache=TRUE}
library(stargazer)
stargazer(scores, nobs = FALSE, mean.sd = TRUE, median = TRUE,
          iqr = TRUE, header = FALSE)
```

## R source code

Please see [Homework 1.rmd](https://github.com/ChristopheHunt/DATA-621-Group-1/blob/master/Homework%201.Rmd) on GitHub for source code.   

https://github.com/ChristopheHunt/DATA-621-Group-1/blob/master/Homework%201.Rmd. 
