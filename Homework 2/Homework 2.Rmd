---
title: "Homework 2"
author: "Group 1"
date: ''
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    includes:
      in_header: header.tex
    number_sections: yes
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: '2'
---


\begin{center}
\topskip0pt
\vspace*{\fill}
\bigskip
\bigskip
\bigskip
Prepared for:\\
\medskip
Dr. Nathan Bastian\\
\smallskip
City University of New York, School of Professional Studies - Data 621\\
\bigskip
Prepared by:\\
\medskip
Group 1\\ 
\medskip
Senthil Dhanapal\\ 
\smallskip
Yadu Chittampalli\\
\smallskip
Christophe Hunt\\  
\vspace*{\fill}
\end{center}


\newpage

```{r, include = FALSE}
library(pacman)
p_load(xtable, knitr, pander, xtable, scales, tidyverse, formatR, caret, pROC)
tidy.opts = list(width.cutoff = 60)
knitr::opts_chunk$set(fig.pos = 'H')
```


# Data Source 

The data is a set of actual classes and predicted classes as provided by Dr. Nathan Bastian for this exercise. We uploaded the data to our public GitHub repository for ease of access. 

```{r load in data, echo=FALSE}
classification <- read.csv("https://raw.githubusercontent.com/ChristopheHunt/DATA-621-Group-1/master/Homework%202/classification-output-data.csv")
```

# Data Explained and Confusion Matrix

We will be using the following columns from the data source: 

* class: the actual class for the observation
* scored.class: the predicted class for the observation (based on a threshold of 0.5)
* scored.probability: the predicted probability of success for the observation

```{r subset data, echo=FALSE, results = 'asis', cache=TRUE}
scores <- subset(classification, select = c("class", "scored.class", "scored.probability"))
```

The raw confusion matrix for our scored data set is represented the following table. The rows represent the actual classes and the columns represent the predicted classes.

```{r, echo=FALSE, results = 'asis', cache=TRUE}
m <- table("Actual" = scores$class, "Predicted" = scores$scored.class)
colnames(m) <- c('Predicted Failure', "Predicted Success")
rownames(m) <- c('Actual Failure', 'Actual Success')
kable(m, align = c("c", "c"))
```

A visual representation of the confusion matrix is presented in the below figure. 

```{r, echo=FALSE, results = 'asis', cache=TRUE, fig.align='center'}
colnames(m) <- c('Predicted Failure', "Predicted \nSuccess\n")
rownames(m) <- c('Actual Failure', 'Actual \nSuccess\n')
mosaicplot(t(m), main="Confusion Matrix Plot", xlab = "", ylab = "")
```


```{r , echo=FALSE}
TP <- function(dfData, predVar, actualVar, posVal, negVal){
  return(length(dfData[which(dfData[predVar]==posVal & dfData[actualVar]==posVal),1]))
}

TN <- function(dfData, predVar, actualVar, posVal, negVal){
  return(length(dfData[which(dfData[predVar]==negVal & dfData[actualVar]==negVal),1]))
}

FP <- function(dfData, predVar, actualVar, posVal, negVal){
  return(length(dfData[which(dfData[predVar]==posVal & dfData[actualVar]==negVal),1]))
}

FN <- function(dfData, predVar, actualVar, posVal, negVal){
  return(length(dfData[which(dfData[predVar]==negVal & dfData[actualVar]==posVal),1]))
}


column_check <- function(df,actual, prediction){
                if (sum(colnames(df) %in% c(actual, prediction)) != 2){
                return("One or more columns were not found, please verify selections")
                }
            }
```

# Function for Accuracy of Predictions 

We developed a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the accuracy of the predictions. 

Accuracy is determined by the below formula: 

$$
\begin{aligned}
Accuracy~=~\frac{True~Positives~+~True~Negatives}{True~Positives~+~False~Positives~+~True~Negatives~+~False~Negatives}\end{aligned}
$$

```{r, echo = FALSE}
Accuracy <- function(dfData, predVar, actualVar, posVal, negVal){
  column_check(dfData, predVar, actualVar)
  tp <- TP(dfData,predVar,actualVar,posVal,negVal)
  tn <- TN(dfData,predVar,actualVar,posVal,negVal)
  fp <- FP(dfData,predVar,actualVar,posVal,negVal)
  fn <- FN(dfData,predVar,actualVar,posVal,negVal)
  
  return ((tp + tn)/(tp + fp + tn + fn))
  
}

```

# Function for Classification Error Rate of Predictions 

We developed a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the classification error rate of the predictions. It also verifies that the accuracy and an error rate sums to one. 

Classification of Error Rate is determined by the below formula:

$$
\begin{aligned}
Classification~Error~Rate~=~\frac{False~Positives~+~False~Negatives}{True~Positives~+~False~Positives~+~True~Negatives~+~False~Negatives}
\end{aligned}
$$

```{r, echo=FALSE}
ClassificationErrorRate <- function(dfData, predVar, actualVar, posVal, negVal){
  column_check(dfData, predVar, actualVar)
  tp <- TP(dfData,predVar,actualVar,posVal,negVal)
  tn <- TN(dfData,predVar,actualVar,posVal,negVal)
  fp <- FP(dfData,predVar,actualVar,posVal,negVal)
  fn <- FN(dfData,predVar,actualVar,posVal,negVal)
  
  return((fp + fn)/(tp + fp + tn + fn))
  }


```

We also check to ensure that the accuracy and the error rate add up to 1. Our results when adding the accuracy and error rate is `r ClassificationErrorRate(scores,'class', 'scored.class', 1, 0) + Accuracy(scores,'class', 'scored.class', 1, 0)` so we are certain our accuracy and error rate are accurate. 

# Function for Precisions of Predictions

Write a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the precision of the predictions. 

Precision is determined by the below formula:

$$
\begin{aligned}
Precision~=~\frac{True~Positives}{True~Positives~+~False~Positives}
\end{aligned}
$$

```{r, echo=FALSE}
Precision <- function(dfData, predVar, actualVar, posVal, negVal){
  column_check(dfData, predVar, actualVar)
  tp <- TP(dfData,predVar,actualVar,posVal,negVal)
  fp <- FP(dfData,predVar,actualVar,posVal,negVal)
  
  return(tp/(tp + fp))
  }

```


# Function for Sensitivity of Predictions

We developed a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the sensitivity of the predictions. Sensitivity is also known as recall. 

Sensitivity is determined by the below formula:

$$
\begin{aligned}
Sensitivity~=~\frac{True~Positives}{True~Positives~+~False~Negatives}
\end{aligned}
$$

```{r, echo = FALSE}
Sensitivity <- function(dfData, predVar, actualVar, posVal, negVal){
  column_check(dfData, predVar, actualVar)
  tp <- TP(dfData,predVar,actualVar,posVal,negVal)
  fn <- FN(dfData,predVar,actualVar,posVal,negVal)
  
  return(tp/(tp + fn))
  }
```

\newpage

# Function for Specificity of Predictions 

We developed a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the specificity of the predictions. 

Specificity is determined by the below formula:

$$
\begin{aligned}
Specificity~=~\frac{True~Negative}{True~Negatives~+~False~Positives}
\end{aligned}
$$

```{r, echo=FALSE}
Specificity <- function(dfData, predVar, actualVar, posVal, negVal){
  column_check(dfData, predVar, actualVar)
  tn <- TN(dfData,predVar,actualVar,posVal,negVal)
  fp <- FP(dfData,predVar,actualVar,posVal,negVal)
  
  return(tn/(tn + fp))
  
}
```


# F1 Score of Predictions 

We developed a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the F1 score of the predictions. 

The F1 score is determined by the below formula: 

$$
\begin{aligned}
F1~Score~=~\frac{2~*Precision~*~Sensitivity}{Precision~+~Sensitivity}
\end{aligned}
$$

```{r, echo = FALSE}
F1Score <- function(dfData, predVar, actualVar, posVal, negVal){
  precision <- Precision(dfData,predVar,actualVar,posVal,negVal)
  sensitivity <- Sensitivity(dfData,predVar,actualVar,posVal,negVal)
  
  return ((2 * precision * sensitivity)/(precision + sensitivity))
  
}
```


# Bounds of F1 Score of Predictions 

The bounds on precision and sensitivity are as follows:

$\textnormal{[0~<~p~<~1]}$

$\textnormal{[0~<~s~<~1~]}$

Both calculated quantities will always be between 0 and 1 because each of them are of the form $\frac{a}{a+b}$.

The formula for the F1 score is as follows:

$F1~=~\frac{2~*~p~*~s}{p~+~s}$

When both calculated quantities are multiplied by each other, we get a value that would be less than those of both calculated quantities. Therefore the product $p*s$ would also be bounded by 0 and 1. 

The denominator in the above equation or the sum of the two calculated quantities would be greater than their product. This implies that $F1$ would most definitely be less than one and greater than 0. 

# Function for ROC curve 

We developed a function that generates an ROC curve from a data set with a true classification column (`class` from our data set) and a probability column (`scored.probability` from our data set). Our function returns a list that includes the plot of the ROC curve and a vector that contains the calculated area under the curve (AUC). As per Dr. Bastion's recommendation we used a sequence of thresholds ranging from 0 to 1 at 0.01 intervals.

```{r, echo=FALSE, eval=TRUE}

ROC <- function(dfData, classVar, probVar, posVal, negVal, thresholds){
        x <- thresholds
        dfForPlot <- data.frame(threshold=integer(), fpr=double(),tpr=double(), auc=double(), dist=double())
        for(i in x)
        {
          plotCoord <- ROC.Coordinates(dfData,classVar, probVar, posVal, negVal, i)
          #print(plotCoord)
          dfForPlot <-  data.frame(rbind(dfForPlot,data.frame(plotCoord)))
        }
        plot(x=dfForPlot$fpr,y=dfForPlot$tpr,lwd=2, type="l", xlab="FPR", ylab="TPR")
        lines(x=c(0, 1), y=c(0, 1), col="black", lwd=1)
        return (dfForPlot)
        
      }


ROC.Coordinates <- function(dfData, classVar, probVar, posVal, negVal, threshold){
                    pred <- rep(negVal,length(dfData[,1]))
                    dfNew <- dfData
                    pred[which(dfData[probVar]>=threshold)]  <- posVal
                    dfNew$pred <- as.factor(pred)
                    
                    sensitivity <- Sensitivity(dfData = dfNew, predVar = "pred",actualVar = classVar,posVal = posVal,negVal = negVal)
                    specificity <- Specificity(dfData = dfNew, predVar = "pred",actualVar = classVar,posVal = posVal,negVal = negVal)
                    
                    tpr <- sensitivity
                    fpr <- 1 - specificity
                    auc <- (sensitivity + specificity)/2
                    dist <- sqrt((1-tpr)^2 + (fpr)^2)
                    
                    return(data.frame(threshold,fpr,tpr,auc,dist))
                    
                  }

calculateAUC <- function(curveInfo){
                tpr1 <- 0
                fpr1 <- 0
                auc <- 0
                curveInfo <- curveInfo[order(-curveInfo$threshold),]
                for(i in 1:nrow(curveInfo))
                {
                  tpr2 <- curveInfo[i,"tpr"]
                  fpr2 <- curveInfo[i,"fpr"]
                  
                  auc <- auc + ((tpr1 + tpr2)/2)*(fpr2-fpr1)
                  
                  
                  tpr1 <- tpr2
                  fpr1 <- fpr2
                }
                return(auc)
              }

```

\newpage

# R Functions created and classification output

##Accuracy of Predictions 

```{r, tidy=TRUE, cache=TRUE, results='asis'}
paste0("Accuracy of Predictions = ", percent(Accuracy(scores,'class', 'scored.class', 1, 0)))
```

##Classification Error Rate of Predictions 

```{r, tidy=TRUE, cache=TRUE, results='asis'}
paste0("Error Rate of Predictions = ", percent(ClassificationErrorRate(scores,'class', 'scored.class', 1, 0)))
```

##Precisions of Predictions
```{r, tidy=TRUE, cache=TRUE, results='asis'}
paste0("Precision of Predictions = ", percent(Precision(scores,'class', 'scored.class', 1, 0)))
```

##Sensitivity of Predictions
```{r, tidy=TRUE, cache=TRUE, results='asis'}
paste0("Sensitivity of Predictions = ", percent(Sensitivity(scores,'class', 'scored.class', 1, 0)))
```

##Specificity of Predictions

```{r, tidy=TRUE, cache=TRUE, results='asis'}
paste0("Specificity of Predictions = ", percent(Specificity(scores,'class', 'scored.class', 1, 0)))
```

\newpage

##F1 Score of Predictions

```{r, tidy=TRUE, cache=TRUE, results='asis'}
paste0("The F1 Score = ", F1Score(scores,'class', 'scored.class', 1, 0))
```

##ROC Function

ROC curve stands for Receiver Operator Characteristic curve used to illustrate the performance of the binary classifier
as its discrimination threshold is varied. The curve is created by plotting True Positive Rate against False Positive Rate at various threshold settings.

We calculated best threshold value using two methods.  
a) by calculating the distance of the point from (0,1).  
b) by calculating AUC by making a curve for threshold{t} by joining points (0,0), (X{t},Y{t}), (1,1)  

Two thresholds intervals were used:  
1) .01   
2) .001   
    
1) As the cut-off interval was set threshold (0,1,0.01) the value returned is very close to the value that the R package `pROC` predicts    

```{r, echo=FALSE}
x <- seq(0,1,0.01)
predVar <- "scored.class"
actualVar <- "class"
posVal <- 1
negVal <- 0
dfData <- scores
classVar <- actualVar
probVar <- "scored.probability"

x <- seq(0,1,0.01)
dfROC <- ROC(dfData,classVar, probVar, posVal, negVal, x)
auc <- calculateAUC(dfROC)
bestValueMethod1 <- dfROC[which(dfROC$dist == min(dfROC$dist)),]
bestValueMethod2 <- dfROC[which(dfROC$auc == max(dfROC$auc)),]
```

AUC using manual calculation is `r auc`.

Best Threshold value using method 1 is `r (sprintf("{Threshold = %f,fpr = %f,tpr = %f,auc = %f,
            dist = %f}", bestValueMethod1[1,1], bestValueMethod1[1,2], bestValueMethod1[1,3],
            bestValueMethod1[1,4], bestValueMethod1[1,5]))`

Best Threshold value using method 2 is `r (sprintf("{Threshold = %f,fpr = %f,tpr = %f,auc = %f,
            dist = %f}", bestValueMethod2[1,1], bestValueMethod2[1,2], bestValueMethod2[1,3],
            bestValueMethod2[1,4], bestValueMethod2[1,5]))`  
  
  
  + 2) when cut-off interval is threshold(0,1,0.001)  -> value is exact to what pROC predicts  

```{r, echo=FALSE}
x <- seq(0,1,0.001)
dfROC <- ROC(scores, 'class', 'scored.probability', 1 ,  0,  x)
auc <- calculateAUC(dfROC)
bestValueMethod1 <- dfROC[which(dfROC$dist == min(dfROC$dist)),]
bestValueMethod2 <- dfROC[which(dfROC$auc == max(dfROC$auc)),]

```

AUC using manual calculation is `r auc`.

Best Threshold value using method 1 is `r (sprintf("{Threshold = %f,fpr = %f,tpr = %f,auc = %f,
            dist = %f}", bestValueMethod1[1,1], bestValueMethod1[1,2], bestValueMethod1[1,3],
            bestValueMethod1[1,4], bestValueMethod1[1,5]))`

Best Threshold value using method 2 is `r (sprintf("{Threshold = %f,fpr = %f,tpr = %f,auc = %f,
            dist = %f}", bestValueMethod2[1,1], bestValueMethod2[1,2], bestValueMethod2[1,3],
            bestValueMethod2[1,4], bestValueMethod2[1,5]))`

\clearpage

#Investigation of `caret` package   

##confusionMatrix function   

The results of the caret confusionMatrix function is below. 

Here, the accuracy is rounded up to 4 decimal places. The confusion matrix has rows that represent the predicted classes and columns that represent the actual classes.    

```{r, echo = FALSE, results='asis'}
library(caret)
options(xtable.comment = FALSE)
`Matrix` <- as.matrix(confusionMatrix(scores[,1], scores[,2])) 
xtable(`Matrix`)
```

## Sensitivity and Specificity functions

Sensitivity and specificity functions in the caret package take in only two inputs - predicted and actual. On the other hand, the sensitivity and specificity functions we created take in 5 outputs - the entire data frame, preidicted variables, actual variables, the positive values, and negative values. 

The results of the Sensitivity function is below: 

```{r, echo = FALSE, results='asis'}
paste0("Sensitivity provided by caret pacakge is ",sensitivity(table(scores[,1], scores[,2])))
```

The results of the Specificity function is below: 

```{r, echo = FALSE, results='asis'}
paste0("Specificity provided by caret pacakge is ",specificity(table(scores[,1], scores[,2])))
```

\pagebreak

# Investigation of the `pROC` R package. 

We used the `pROC` R package to generate an ROC curve for the data set. 

```{r, warning=FALSE, message=FALSE, echo=FALSE, results='markup'}
library(pROC)
roc_curve <- roc(dfData$class, dfData$scored.probability)
plot(roc_curve)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE, results='asis'}
bestPROC <- coords(roc_curve, "best", ret=c("threshold", "1-specificity", "sensitivity"))
```

Best Threshold value using pROC package is `r (sprintf("{Threshold = %f,fpr = %f,tpr = %f}", 
            bestPROC[1], bestPROC[2], bestPROC[3]))`

Note: The second method (using auc) predicts better than first method (using distance from (0,1))

\newpage

# Appendix A

## Session Info

```{r, results='asis', echo=FALSE, eval = TRUE}
toLatex(sessionInfo())
```

## Data Table

```{r, results='asis', echo=FALSE, cache=TRUE}
kable(scores, align = c("c","c","c"))
```

## R source code

Please see [Homework 2.rmd](https://github.com/ChristopheHunt/DATA-621-Group-1/blob/master/Homework%202/Homework%202.Rmd) on GitHub for source code.   

https://github.com/ChristopheHunt/DATA-621-Group-1/blob/master/Homework%202/Homework%202.Rmd. 
