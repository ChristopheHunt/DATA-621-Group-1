---
title: "Homework 2"
author: "Group 1"
date: ''
output:
  pdf_document:
    includes:
      in_header: header.tex
    number_sections: yes
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: '2'
---

\begin{center}
\topskip0pt
\vspace*{\fill}
\bigskip
\bigskip
\bigskip
Prepared for:\\
\medskip
Dr. Nathan Bastian\\
\smallskip
City University of New York, School of Professional Studies - Data 621\\
\bigskip
Prepared by:\\
\medskip
Group 1\\ 
\medskip
Senthil Dhanapal\\ 
\smallskip
Yadu Chittampalli\\
\smallskip
Christophe Hunt\\  
\vspace*{\fill}
\end{center}


\newpage

```{r, include = FALSE}
library(pacman)
p_load(xtable, knitr, pander, xtable, scales, tidyverse, formatR, caret, pROC)
tidy.opts = list(width.cutoff = 60)
```


# Data Source 

The data is a set of actual classes and predicted classes as provided by Dr. Nathan Bastian for this exercise. We uploaded the data to our public GitHub repository for ease of access. 

```{r load in data, echo=FALSE}
classification <- read.csv("https://raw.githubusercontent.com/ChristopheHunt/DATA-621-Group-1/master/Homework%202/classification-output-data.csv")
```

# Data Explained and Confusion Matrix

We will be using the following columns from the data source: 

* class: the actual class for the observation
* scored.class: the predicted class for the observation (based on a threshold of 0.5)
* scored.probability: the predicted probability of success for the observation

```{r subset data, echo=FALSE, results = 'asis', cache=TRUE}
scores <- subset(classification, select = c("class", "scored.class", "scored.probability"))
```

The raw confusion matrix for our scored data set is represented the following table. The rows represent the actual classes and the columns represent the predicted classes.

```{r, echo=FALSE, results = 'asis', cache=TRUE}
m <- table("Actual" = scores$class, "Predicted" = scores$scored.class)
colnames(m) <- c('Predicted Failure', "Predicted Success")
rownames(m) <- c('Actual Failure', 'Actual Success')
kable(m, align = c("c", "c"))
```

A visual representation of the confusion matrix is presented in the below figure. 

```{r, echo=FALSE, results = 'asis', cache=TRUE, fig.align='center'}
colnames(m) <- c('Predicted Failure', "Predicted \nSuccess\n")
rownames(m) <- c('Actual Failure', 'Actual \nSuccess\n')
mosaicplot(t(m), main="Confusion Matrix Plot", xlab = "", ylab = "")
```


```{r , echo=FALSE}
TruePositive <- function(df, prediction, actual, positiveValue, negativeValue){
                return(length(df[which(df[prediction] == positiveValue & df[actual] == positiveValue),1]))
                }

TrueNegative <- function(df, prediction, actual, positiveValue, negativeValue){
                return(length(df[which(df[prediction] == negativeValue & df[actual] == negativeValue),1]))
                }

FalsePositive <- function(df, prediction, actual, positiveValue, negativeValue){
                 return(length(df[which(df[prediction] == positiveValue & df[actual] == negativeValue),1]))
                 }

FalseNegative <- function(df, prediction, actual, positiveValue, negativeValue){
                 return(length(df[which(df[prediction] == negativeValue & df[actual] == positiveValue),1]))
                }

column_check <- function(df,actual, prediction){
                if (sum(colnames(df) %in% c(actual, prediction)) != 2){
                return("One or more columns were not found, please verify selections")
                }
            }
```

# Function for Accuracy of Predictions 

We developed a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the accuracy of the predictions. 

Accuracy is determined by the below formula: 

$$
\begin{aligned}
Accuracy~=~\frac{True~Positives~+~True~Negatives}{True~Positives~+~False~Positives~+~True~Negatives~+~False~Negatives}\end{aligned}
$$

```{r, echo = FALSE}
Accuracy <- function(df, actual, prediction, positiveValue, negativeValue){
  column_check(df, actual, prediction)
  accuracy <- (TruePositive(df, actual, prediction, positiveValue, negativeValue) + 
               TrueNegative(df, actual, prediction, positiveValue, negativeValue)) / nrow(df) 
  return(accuracy)
}

```

# Function for Classification Error Rate of Predictions 

We developed a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the classification error rate of the predictions. It also verifies that the accuracy and an error rate sums to one. 

Classification of Error Rate is determined by the below formula:

$$
\begin{aligned}
Classification~Error~Rate~=~\frac{False~Positives~+~False~Negatives}{True~Positives~+~False~Positives~+~True~Negatives~+~False~Negatives}
\end{aligned}
$$

```{r, echo=FALSE}
ClassificationErrorRate <- function(df, actual, prediction, positiveValue, negativeValue){
  column_check(df, actual, prediction)
  error <- (FalsePositive(df, actual, prediction, positiveValue, negativeValue) + 
            FalseNegative(df, actual, prediction, positiveValue, negativeValue)) / nrow(df)
  if (error + Accuracy(df, actual, prediction, positiveValue, negativeValue) != 1){
    return("Accuracy and Error Rate does not sum to 1")
    }
  return(error)
}

```

# Function for Precisions of Predictions

Write a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the precision of the predictions. 

Precision is determined by the below formula:

$$
\begin{aligned}
Precision~=~\frac{True~Positives}{True~Positives~+~False~Positives}
\end{aligned}
$$

```{r, echo=FALSE}
Precision <- function(df, actual, prediction, positiveValue, negativeValue){
  column_check(df, actual, prediction)
  tp <- TruePositive(df, actual, prediction, positiveValue, negativeValue)
  fp <- FalsePositive(df, actual, prediction, positiveValue, negativeValue)
  return(tp/(tp + fp))
 }
```


# Function for Sensitivity of Predictions

We developed a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the sensitivity of the predictions. Sensitivity is also known as recall. 

Sensitivity is determined by the below formula:

$$
\begin{aligned}
Sensitivity~=~\frac{True~Positives}{True~Positives~+~False~Negatives}
\end{aligned}
$$

```{r, echo = FALSE}
Sensitivity <- function(df, actual, prediction, positiveValue, negativeValue){
  column_check(df, actual, prediction)
  tp <- TruePositive(df, actual, prediction, positiveValue, negativeValue)
  fn <- FalseNegative(df, actual, prediction, positiveValue, negativeValue)
  return(tp/(tp + fn))
}
```

\newpage

# Function for Specificity of Predictions 

We developed a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the specificity of the predictions. 

Specificity is determined by the below formula:

$$
\begin{aligned}
Specificity~=~\frac{True~Negative}{True~Negatives~+~False~Positives}
\end{aligned}
$$

```{r, echo=FALSE}
Specificity <- function(df, actual, prediction, positiveValue, negativeValue){
  column_check(df, actual, prediction)
  tn <- TrueNegative(df, actual, prediction, positiveValue, negativeValue)
  fp <- FalsePositive(df, actual, prediction, positiveValue, negativeValue)
  return(tn/(tn + fp))
}
```


# F1 Score of Predictions 

We developed a function that takes the data set as a dataframe, with actual and predicted classifications identified,
and returns the F1 score of the predictions. 

The F1 score is determined by the below formula: 

$$
\begin{aligned}
F1~Score~=~\frac{2~*Precision~*~Sensitivity}{Precision~+~Sensitivity}
\end{aligned}
$$

```{r, echo = FALSE}
f1_score <- function(df, actual, prediction, positiveValue, negativeValue){
  column_check(df, actual, prediction)
  p <- Precision(df, actual, prediction, positiveValue, negativeValue)
  s <- Sensitivity(df, actual, prediction, positiveValue, negativeValue)
  f1_score <- ((2 * p * s) / (p + s))
  return(f1_score)
  }
```


# Bounds of F1 Score of Predictions 

Before we move on, let's consider a question that was asked: What are the bounds on the F1 score? Show
that the F1 score will always be between 0 and 1. (Hint: If 0 < ???? < 1 and 0 < ???? < 1 then ????????- Christophe

# Function for ROC curve 

We developed a function that generates an ROC curve from a data set with a true classification column (`class` from our data set) and a probability column (`scored.probability` from our data set). Our function returns a list that includes the plot of the ROC curve and a vector that contains the calculated area under the curve (AUC). As per Dr. Bastion's recommendation we used a sequence of thresholds ranging from 0 to 1 at 0.01 intervals.

```{r, echo=FALSE, eval=FALSE}
ROC.Coordinates <- function(df, classVar, probVar, positiveValue, negativeValue, threshold){

                    pred <- rep(negativeValue,length(df[,1]))
                    dfNew <- df
                    pred[which(df[probVar]>=threshold)]  <- positiveValue
                    dfNew$pred <- as.factor(pred)

                    sensitivity <- Sensitivity(df = dfNew, actual = classVar, prediction = "pred", positiveValue, negativeValue)
                    specificity <- Specificity(df = dfNew, actual = classVar, prediction = "pred", positiveValue, negativeValue)
              
                    tpr  <- sensitivity
                    fpr  <- (1 - specificity)
                    auc  <- ((sensitivity + specificity) / 2)
                    dist <- (sqrt((1 - tpr) ^ 2 + (fpr) ^ 2))
                  
                    return(data.frame(threshold,fpr,tpr,auc,dist))
                    }

ROC <- function(df, classVar, probVar, positiveValue, negativeValue, thresholds){
                x <- thresholds
                dfForPlot <- data.frame(threshold = integer(), fpr = double(),tpr = double(), auc = double(), dist = double())
                for (i in x){
                  plotCoord <- ROC.Coordinates(df,classVar, probVar, positiveValue, negativeValue, i)
                  dfForPlot <-  data.frame(rbind(dfForPlot,data.frame(plotCoord)))
                }
                plot(x = dfForPlot$fpr,y = dfForPlot$tpr,lwd = 2, type = "l", xlab = "FPR", ylab = "TPR")
                lines(x = c(0, 1), y = c(0, 1), col = "black", lwd = 1)
                return(dfForPlot)
              }

calculateAUC <- function(curveInfo){
                  tpr1 <- 0
                  fpr1 <- 0
                  auc  <- 0
                  curveInfo <- curveInfo[order(-curveInfo$threshold),]
                  for (i in 1:nrow(curveInfo)){     
                    tpr2 <- curveInfo[i,"tpr"]
                    fpr2 <- curveInfo[i,"fpr"]
                    auc  <- auc + ((tpr1 + tpr2)/2) * (fpr2 - fpr1)
                    tpr1 <- tpr2
                    fpr1 <- fpr2
                    }
                  return(auc)
                }
```

\newpage

# R Functions created and classification output

##Accuracy of Predictions 

```{r, tidy=TRUE, cache=TRUE, results='asis'}
paste0("Accuracy of Predictions = ", percent(Accuracy(df = scores, actual = 'class', prediction = 'scored.class', positiveValue = 1, negativeValue = 0)))
```

##Classification Error Rate of Predictions 

```{r, tidy=TRUE, cache=TRUE, results='asis'}
paste0("Error Rate of Predictions = ", percent(ClassificationErrorRate(df = scores, actual = 'class', prediction = 'scored.class', positiveValue = 1, negativeValue = 0)))
```

##Precisions of Predictions
```{r, tidy=TRUE, cache=TRUE, results='asis'}
paste0("Precision of Predictions = ", percent(Precision(df = scores, actual = 'class', prediction = 'scored.class', positiveValue = 1, negativeValue = 0)))
```

##Sensitivity of Predictions
```{r, tidy=TRUE, cache=TRUE, results='asis'}
paste0("Sensitivity of Predictions = ", percent(Sensitivity(df = scores, actual = 'class', prediction = 'scored.class', positiveValue = 1, negativeValue = 0)))
```

##Specificity of Predictions

```{r, tidy=TRUE, cache=TRUE, results='asis'}
paste0("Specificity of Predictions = ", percent(Specificity(df = scores, actual = 'class', prediction = 'scored.class', positiveValue = 1, negativeValue = 0)))
```

\newpage

##F1 Score of Predictions

```{r, tidy=TRUE, cache=TRUE, results='asis'}
paste0("The F1 Score = ", f1_score(df = scores, actual = 'class', prediction = 'scored.class', positiveValue = 1, negativeValue = 0))
```

##ROC Function

We calculated best threshold value using two methods.  
a) by calculating the distance of the point from (0,1).  
b) by calculating AUC by making a curve for threshold{t} by joining points (0,0), (X{t},Y{t}), (1,1)  

Two thresholds intervals were used:  
1) .01   
2) .001   
    
1) As the cut-off interval was set threshold (0,1,0.01) the value returned is very close to the value that the R package `pROC` predicts    

```{r, echo=FALSE}
x <- seq(0,1,0.01)
dfROC <- ROC(dfData = scores, classVar = 'class', probVar = 'scored.probability', posVal = 1, negVal = 0, thresholds = x)
auc <- calculateAUC(dfROC)
bestValueMethod1 <- dfROC[which(dfROC$dist == min(dfROC$dist)),]
bestValueMethod2 <- dfROC[which(dfROC$auc == max(dfROC$auc)),]
```

AUC using manual calculation is `r auc`.

Best Threshold value using method 1 is `r (sprintf("{Threshold = %f,fpr = %f,tpr = %f,auc = %f,
            dist = %f}", bestValueMethod1[1,1], bestValueMethod1[1,2], bestValueMethod1[1,3],
            bestValueMethod1[1,4], bestValueMethod1[1,5]))`

Best Threshold value using method 2 is `r (sprintf("{Threshold = %f,fpr = %f,tpr = %f,auc = %f,
            dist = %f}", bestValueMethod2[1,1], bestValueMethod2[1,2], bestValueMethod2[1,3],
            bestValueMethod2[1,4], bestValueMethod2[1,5]))`  
  
  
  + 2) when cut-off interval is threshold(0,1,0.001)  -> value is exact to what pROC predicts  

```{r, echo=FALSE}
x <- seq(0,1,0.001)
dfROC <- ROC(df = scores, classVar = 'class', probVar = 'scored.probability', positiveValue = 1 , negativeValue = 0, thresholds = x)
auc <- calculateAUC(dfROC)
bestValueMethod1 <- dfROC[which(dfROC$dist == min(dfROC$dist)),]
bestValueMethod2 <- dfROC[which(dfROC$auc == max(dfROC$auc)),]
```

AUC using manual calculation is `r auc`.

Best Threshold value using method 1 is `r (sprintf("{Threshold = %f,fpr = %f,tpr = %f,auc = %f,
            dist = %f}", bestValueMethod1[1,1], bestValueMethod1[1,2], bestValueMethod1[1,3],
            bestValueMethod1[1,4], bestValueMethod1[1,5]))`

Best Threshold value using method 2 is `r (sprintf("{Threshold = %f,fpr = %f,tpr = %f,auc = %f,
            dist = %f}", bestValueMethod2[1,1], bestValueMethod2[1,2], bestValueMethod2[1,3],
            bestValueMethod2[1,4], bestValueMethod2[1,5]))`

# Investigation of `caret` package. 

In particular, consider the functions confusionMatrix, sensitivity, and specificity. Apply the functions to the data set. How do the results compare with your own functions? 

```{r, echo = FALSE, results='asis'}
library(caret)
as.matrix(confusionMatrix(scores[,1], scores[,2]))
```

Here, the accuracy is rounded up to 4 decimal places. The confusion matrix has rows that represent the predicted classes and columns that represent the actual classes. 

```{r, echo = FALSE, results='asis'}
sensitivity(table(scores[,1], scores[,2]))
```


```{r, echo = FALSE, results='asis'}
specificity(table(scores[,1], scores[,2]))
```

# Investigation of the `pROC` R package. 

We used the `pROC` R package to generate an ROC curve for the data set. 

```{r,warning=FALSE,message=FALSE, echo=FALSE, results='asis'}
roc_curve <- roc(scores$class, scores$scored.probability)
plot(roc_curve)
bestPROC <- coords(roc_curve, "best", ret = c("threshold", "1-specificity", "sensitivity"))
```

Best Threshold value using pROC package is `r (sprintf("{Threshold = %f,fpr = %f,tpr = %f}", 
            bestPROC[1], bestPROC[2], bestPROC[3]))`

Our second method (using `auc`) predicts better than first method (using distance from (0,1))

\newpage

# Appendix A

## Session Info

```{r, results='asis', echo=FALSE, eval = TRUE}
toLatex(sessionInfo())
```

## Data Table

```{r, results='asis', echo=FALSE, cache=TRUE}
kable(scores, align = c("c","c","c"))
```

## R source code

Please see [Homework 2.rmd](https://github.com/ChristopheHunt/DATA-621-Group-1/blob/master/Homework%202/Homework%202.Rmd) on GitHub for source code.   

https://github.com/ChristopheHunt/DATA-621-Group-1/blob/master/Homework%202/Homework%202.Rmd. 
