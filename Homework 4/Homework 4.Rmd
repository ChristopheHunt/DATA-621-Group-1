---
title: "Homework 4"
author: "Group 1"
date: ''
output:
  pdf_document:
    includes:
      in_header: header.tex
    number_sections: yes
    toc: yes
    toc_depth: 2
  html_document:
    toc: yes
    toc_depth: '2'
---

```{r programming style guide, echo=FALSE, eval=FALSE}
##Style guide
##USE CAMEL CASING!! https://en.wikipedia.org/wiki/Camel_case, start with lowercase then each word after start with upper case
##upper case for vector, lists, etc. ex. X <- c("yes","no","true","false")
##lower case for scalar/single value, ex. x <- 1
##data frame start with df followed by description, ex. dfEval <- evaluation data frame
##assignments use "<-" not "="
```

```{r library load, include = FALSE}
library(pacman)
p_load(Hmisc, xlsx, xtable, knitr, scales, magrittr, tidyverse, stringr, e1071, corrplot, knitcitations, bibtex, missForest, abc, foreach, doParallel, stargazer, forecast, matrixStats, glmulti, leaps, data.table, highlight, car, pracma)
```

\begin{center}
\bigskip
\bigskip
\bigskip
Prepared for:\\
\medskip
Dr. Nathan Bastian\\
\smallskip
City University of New York, School of Professional Studies - Data 621\\
\bigskip
Prepared by:\\
\medskip
Group 1\\ 
\medskip
Senthil Dhanapal\\ 
\smallskip
Yadu Chittampalli\\
\smallskip
Christophe Hunt\\  

\end{center}

\newpage

# Introduction

Consumers who own a car are often required to purchase car insurance to protect themselves from serious financial repercussions of being involved in a car accident. Insurance Providers must determine the risk of offering insurance coverage to a new customer through accurate statistical models that evaluate the consumers propensity for accidents. Since Insurance Providers are motivated by collecting the maximum amount of revenue from consumers while returning the lowest amount in accident claims, statistical modeling provides Insurance Providers with insight into the consumers behavior and the most appropriate pricing schemes[^1]. 

[^1]: ["Insider Information: How Insurance Companies Measure Risk - Insurance Companies.com."](http://www.insurancecompanies.com/insider-information-how-insurance-companies-measure-risk/) Insurance Companiescom. N.p., n.d. Web. 06 Nov. 2016.

# Statement of the Problem

The purpose of this report is to develop statistical models to make inference into the likelihood of a customer being involved in a car accident and the cost associated of a customer being involved in a car accident. 

# Data Exploration  

## Variables Explained

The variables provided in the `Insurance Training Data Set` are explained below:

```{r data dictionary load, include=FALSE, cache=TRUE}
download.file("https://github.com/ChristopheHunt/DATA-621-Group-1/raw/master/Homework%204/Data%20Dictionary.xlsx?raw=true", destfile = "Data Dictionary HW4 - Group 1.xlsx", mode = 'wb')
daDict <- read.xlsx("Data Dictionary HW4 - Group 1.xlsx", sheetIndex = 1)
file.remove(dir(getwd(), pattern = "Data Dictionary HW4 - Group 1", full.names = TRUE))
```
\footnotesize
```{r data dictionary table, eval=TRUE, results='asis', echo=FALSE}
kable(daDict %>% 
      mutate(`Variable Code` = VARIABLE.NAME, Definition = DEFINITION)  %>%
      dplyr::select(`Variable Code`, Definition),
      align = c("c","l"))
```


```{r data prep, echo=FALSE, results='asis'}
dfTr <- read.csv("https://raw.githubusercontent.com/ChristopheHunt/DATA-621-Group-1/master/Homework%204/insurance_training_data.csv")

removeDollars <- function(column){
             as.numeric(gsub("\\,","",gsub("\\$","", column)))
            }

dfTr$JOB <- as.character(dfTr$JOB)

dfTr$JOB[(dfTr$JOB == "")] <- NA

dfTr <- dfTr %>%
        mutate(INCOME = removeDollars(INCOME),
               HOME_VAL = removeDollars(HOME_VAL),
               BLUEBOOK = removeDollars(BLUEBOOK),
               OLDCLAIM = removeDollars(OLDCLAIM),
               MSTATUS = as.factor(gsub("z_", "", MSTATUS)),
               SEX = as.factor(gsub("z_", "", SEX)),
               EDUCATION = factor((gsub("<", "Less Than ", gsub("z_", "", EDUCATION))), 
                                  levels = c("Less Than High School", "High School", "Bachelors", "Masters", "PhD")),
               JOB = as.factor(gsub("z_", "", JOB)),
               CAR_TYPE = as.factor(gsub("z_", "", CAR_TYPE)),
               URBANICITY = as.factor(gsub("z_", "", URBANICITY)))

```
\normalsize

\newpage

### Nominal Variables

We first look at our nominal variables and their applicable proportions. Interestingly, we see that in this data set only a quarter of the customer records indicate an accident occurred. Also, the majority of consumers in this data set have no kids at home, are married, more than a high school education but less than a PhD, use their car for private purposes, typically own a SUV or minivan, and also live in an urban environment. This provides an interesting insight to the type of customer this data set represents and should be considered when further interpreting our statistical model. Additionally, we should be mindful of any selection biases in this data set as consumers with extremely risky histories are likely to have not been extended insurance coverage. 

```{r nominal variables, echo=FALSE, results='asis', cache=TRUE}
options(xtable.comment = FALSE)

reporttools::tableNominal(dfTr %>% 
                        dplyr::select(TARGET_FLAG, KIDSDRIV, HOMEKIDS, PARENT1, MSTATUS, SEX, 
                                      EDUCATION, JOB, CAR_USE,  CAR_TYPE,RED_CAR, CLM_FREQ, REVOKED, URBANICITY), 
                        cap = "Table of nominal variables", lab = "tab: nominal", print.pval = "fisher",
                        caption.placement = "top", longtable = TRUE, 
                        add.to.row = list(pos = list(0), command = "\\hline \\endhead "))
```

### Continuous and Discrete Variables

We can see that in our continuous and discrete variables there is some additional variability. The median claim amount (`TARGET_AMT`) is 0 which would coincide with only a quarter for records indicating an accident. However, the spread is large since the average payout is only \$1,504.30 but the maximum payout was \$107,586.10. Surprisingly, the median `AGE` is 45 and the average `AGE` is 44.8 years, while we expected a lower average it could be due to simple selection bias in the data set source or the aging US population bringing this average higher [^3]. We also noticed that an `INCOME` of \$0.00 seems unwise because it is unclear how the individual would be able to cover their premium costs without parental support. Finally, we should note that the data set has as `CAR_AGE` of -3, which is impossible and will need to be removed.

There are many missing values for this portion of our data set, we have over 400 values missing for years on the job, income, home value, and car age. Due to these missing values we will need to impute to complete our statistical model. 

[^3]: Ortman, Jennifer M., Victoria A. Velkoff, and Howard Hogan. "An aging nation: the older population in the United States." Washington, DC: US Census Bureau (2014): 25-1140.

```{r continous variables, echo=FALSE, results='asis', cache=TRUE}
reporttools::tableContinuous(dfTr %>% 
                               dplyr::select(TARGET_AMT, TIF, AGE, YOJ, INCOME, HOME_VAL,
                                             TRAVTIME, BLUEBOOK, OLDCLAIM, MVR_PTS, CAR_AGE), 
                             longtable = TRUE)
```

\newpage

## Imputting Missing Values

In order to address the missing values in our variables we used a non-parametric imputation method (Random Forest) using the `missForest` package. The function is particularly useful in that it can handle any type of input data and it will make as few assumptions about the structure of the data as possible.[^2]

[^2]: Stekhoven, Daniel J., and Peter Bühlmann. ["MissForest-non-parametric missing value imputation for mixed-type data." Bioinformatics 28.1 (2012): 112-118](http://bioinformatics.oxfordjournals.org/content/28/1/112.short).

```{r, results='asis', echo = FALSE, include=FALSE, cache=TRUE, eval=FALSE}
registerDoParallel(cl = makeCluster(10), cores = 2)

set.seed(1234)

dfTr$CAR_AGE[(dfTr$CAR_AGE == -3)] <- NA

imputed_data <- dfTr %>% missForest(maxiter = 10, ntree = 100, replace = TRUE, parallelize = 'forests', verbose = TRUE) #Takes approximately 1 hour to process

write.csv(imputed_data$ximp,"imputed_insurance_training_data.csv", row.names = FALSE) #wrote imputed_data to csv file due to processing time taken by missForest
```

```{r, results='asis', echo = FALSE, cache=TRUE, eval=TRUE}
imputedDfTr <- read.csv("https://raw.githubusercontent.com/ChristopheHunt/DATA-621-Group-1/master/Homework%204/imputed_insurance_training_data.csv")

imputedDfTr <- imputedDfTr %>% 
               dplyr::select(-INDEX)

digits = 1

descriptive <- describe(imputedDfTr,
                        descript = "Table 2 : Imputed Descriptive Statistics", 
                        digits = digits)

latex(descriptive, file = '')
```

## Exploration of Variables

```{r correlation matrix, fig.cap= "Correlation Plot of Training Data Set", echo = FALSE, cache=TRUE, eval=TRUE}

imputedDfTrMx <- imputedDfTr %>%
                   mutate(Yes = 1) %>%
                   spread(EDUCATION, Yes, fill = 0)  %>%
                   mutate(Yes = 1) %>%
                   spread(CAR_TYPE, Yes, fill = 0) %>%
                   mutate(Yes = 1) %>%
                   mutate(SEX = ifelse(SEX == "F", "Female", "Male")) %>%
                   spread(SEX, Yes, fill = 0) %>%
                   mutate(Yes = 1) %>%
                   mutate(CAR_USE = ifelse(CAR_USE == "Commercial", "Commercial Car Use", "Private Car Use")) %>%
                   spread(CAR_USE, Yes, fill = 0) %>%
                   mutate(Yes = 1) %>%
                   spread(URBANICITY, Yes, fill = 0 ) %>%
                   mutate(PARENT1 = ifelse(PARENT1 == "No", 0, 1),
                          MSTATUS = ifelse(MSTATUS == "No", 0, 1),
                          RED_CAR = ifelse(RED_CAR == "no", 0, 1),
                          REVOKED = ifelse(REVOKED == "No", 0, 1)) %>%
                  mutate(Yes = 1) %>%
                  spread(JOB, Yes, fill = 0)  

imputedDfTrMx <- as.matrix(imputedDfTrMx)

corMx <- cor(imputedDfTrMx , use = "everything",  method = c("pearson"))
corrplot(corMx, order = "hclust", addrect = 10, method = "square", tl.col = "black", tl.cex = .50, na.label = " ")
```

# Data Transformation

## Outliers Treatment

We chose winsorizing as the method to address outliers. Instead of trimming values, winsorizing uses the interquantile range to replace values that are above or below the interquantile range multiplied by a factor. Those values above or below the range multiplied by the factor are then replaced with max and min value of the interquantile range. Using the factor 2.2 for winsorizing outliers is a method developed my Hoaglin and Iglewicz and published Journal of American Statistical Association in 1987[^7].  

[^7]:Hoaglin, D. C., and Iglewicz, B. (1987), Fine tuning some resistant rules for outlier labeling, Journal of American Statistical Association, 82, 1147-1149.

The below table is the summary results of the winsorizing of the data. 


```{r winsorize dataframe,  echo=FALSE, results='asis',  cache=TRUE, eval=TRUE}
library(forecast)

winsorize <- function(x, multiple=2.2)
{
  q <- quantile(x)
  iqr <- IQR(x)
  iqrAdjusted <- iqr*multiple

  rangeLow <- q['25%']-iqrAdjusted
  rangeHigh <- q['75%']+iqrAdjusted

  x[x<rangeLow] <- min(x[x>rangeLow])
  x[x>rangeHigh] <- max(x[x<rangeHigh])

  return(x)  

}
library(forecast)

winsorizedDataset <- function(dataset){
  dataset <- dataset[,1:ncol(dataset)]
  wdataset <- matrix(data = 0, nrow = nrow(dataset), ncol = ncol(dataset))
  #l1 <- numeric(ncol(dataset))
  for (i in 1:ncol(dataset)){
    if(i == 1 || i == 2 || i == 3 || i == 5 || i == 8 || i == 10 || i == 11 || i == 12 || i == 13 || i == 15 || i == 18 || i == 19 || i == 21 || i == 22 || i == 23 || i == ncol(dataset)){
      wdataset[,i] <- dataset[,i]
    }
    else {
    wdataset[,i] <- winsorize(dataset[,i], multiple = 2.2)
    wdataset[,i] <- as.numeric(wdataset[,i])
    #l1[i] <- BoxCox.lambda(wdataset[,i])
    }
    colnames(wdataset) <- colnames(dataset)
  }
  return(data.frame(wdataset))
}

wimputedDfTr <- imputedDfTr #winsorizedDataset(imputedDfTr)

stargazer(wimputedDfTr, header = FALSE)
```

## BoxCox Transformations 

The Box-Cox transformations were done only on three of the input variables - income, house value, and the total number of claims during the past 5 years. These transformations were done based on the residual plots. In the residual plots, these three variables showed a great deal of non-constant variance because the plots were funnel-shaped. 

```{r, echo=FALSE, cache=TRUE}
library(car)
fit <- lm(TARGET_AMT ~ KIDSDRIV + AGE + HOMEKIDS + YOJ + INCOME + PARENT1 + HOME_VAL + MSTATUS + SEX + EDUCATION + JOB + TRAVTIME + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + RED_CAR + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + URBANICITY, data=dfTr)

ncvTest(fit, ~ KIDSDRIV + AGE + HOMEKIDS + YOJ + INCOME + PARENT1 + HOME_VAL + MSTATUS + SEX + EDUCATION + JOB + TRAVTIME + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + RED_CAR + OLDCLAIM + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + URBANICITY, data=dfTr)
```

```{r, echo=FALSE,cache=TRUE}
residualPlots(fit)
```

Using the `BoxCox.lambda` function from the `forecast` package we are able to determine our necessary transformations to our independent variables. 

```{r, echo=FALSE, cache=TRUE}
l1 <- BoxCox.lambda(as.numeric(wimputedDfTr$INCOME))
l2 <- BoxCox.lambda(as.numeric(wimputedDfTr$HOME_VAL))
l3 <- BoxCox.lambda(as.numeric(wimputedDfTr$OLDCLAIM))

lamdas <- c(l1, l2, l3)
Variables <- c("INCOME", "HOME_VAL", "OLDCLAIM")
dfBoxCox <- as.data.frame(cbind(lamdas, Variables))

colnames(dfBoxCox) <- c("$\\lambda$", "Variables")

kable(dfBoxCox, align = c("c", "c"))
```


Utilizing the below table of common transformations based on the lambda value of the BoxCox we further transform our independent variables.

\centering

Common Box-Cox Transformations[^8] [^4]

\setlength{\tabcolsep}{12pt}

\begin{tabular}{ c c }
\hline
$\lambda$ & Y' \\ \hline
-2 &	$Y^{-2}~=~\frac{1}{Y^{2}}$ \\
-1 &	$Y^{-1}~=~\frac{1}{Y^{1}}$ \\
-0.5 &	$Y^{-0.5}~=~\frac{1}{\sqrt{(Y)}}$ \\
0	& $\log(Y)$ \\
.25  & $\sqrt[4]{Y}$\\  
0.5	& $Y^{0.5}~=~\sqrt{(Y)}$ \\
1	& $Y^{1}~=~Y$ \\
2	& $Y^{2}$ \\

\end{tabular}



\justifying

Lambda values were truncated to the nearest tenth that match a common transformation as per the below table.

\centering

\begin{tabular}{ c c }
\hline
variable & variable transformation \\ \hline
INCOME & $\sqrt[4]{INCOME}$ \\
HOME VAL & $\sqrt(HOME~VAL)$ \\
OLDCLAIM & $\log{(OLDCLAIM)}$ 
\end{tabular}

\justifying

\setlength{\tabcolsep}{6pt}

[^4]: Osborne, Jason W. "Improving your data transformations: Applying the Box-Cox transformation." Practical Assessment, Research & Evaluation 15.12 (2010): 1-9.

[^8]: [By Understanding Both the Concept of Transformation and the Box-Cox Method, Practitioners Will Be Better Prepared to Work with Non-normal Data.](https://www.isixsigma.com/tools-templates/normality/making-data-normal-using-box-cox-power-transformation/) . "Making Data Normal Using Box-Cox Power Transformation." ISixSigma. N.p., n.d. Web. 29 Oct. 2016.

\newpage

# Models Built

```{r model metrics, echo=FALSE,  cache=TRUE}
#Supporting Functions

TP <- function(df, predResp, resp, p, n)
{
  return(length(df[which(df[predResp]==p & df[resp]==p),1]))
}

TN <- function(df, predResp, resp, p, n)
{
  return(length(df[which(df[predResp]==n & df[resp]==n),1]))
}

FP <- function(df, predResp, resp, p, n)
{
  return(length(df[which(df[predResp]==p & df[resp]==n),1]))
}

FN <- function(df, predResp, resp, p, n)
{
  return(length(df[which(df[predResp]==n & df[resp]==p),1]))
}


Accuracy <- function(df, predResp, resp, p, n)
{
  tp <- TP(df,predResp,resp,p,n)
  tn <- TN(df,predResp,resp,p,n)
  fp <- FP(df,predResp,resp,p,n)
  fn <- FN(df,predResp,resp,p,n)
  
  return ((tp + tn)/(tp + fp + tn + fn))
  
}

ClassificationErrorRate <- function(df, predResp, resp, p, n)
{
  tp <- TP(df,predResp,resp,p,n)
  tn <- TN(df,predResp,resp,p,n)
  fp <- FP(df,predResp,resp,p,n)
  fn <- FN(df,predResp,resp,p,n)
  
  return ((fp + fn)/(tp + fp + tn + fn))
  
}

Precision <- function(df, predResp, resp, p, n)
{
  tp <- TP(df,predResp,resp,p,n)
  fp <- FP(df,predResp,resp,p,n)
  
  return (tp/(tp + fp))
  
}

Sensitivity <- function(df, predResp, resp, p, n)
{
  tp <- TP(df,predResp,resp,p,n)
  fn <- FN(df,predResp,resp,p,n)
  
  return (tp/(tp + fn))
  
}

Specificity <- function(df, predResp, resp, p, n)
{
  tn <- TN(df,predResp,resp,p,n)
  fp <- FP(df,predResp,resp,p,n)
  
  return (tn/(tn + fp))
  
}

F1Score <- function(df, predResp, resp, p, n)
{
  precision <- Precision(df,predResp,resp,p,n)
  sensitivity <- Sensitivity(df,predResp,resp,p,n)
  
  return ((2 * precision * sensitivity)/(precision + sensitivity))
  
}


ROC <- function(df, respProb, resp, p, n, thresholds, plotTitle)
{
  x <- thresholds
  dfForPlot <- data.frame(threshold=integer(), fpr=double(),tpr=double(), auc=double(), dist=double())
  for(i in x)
  {
    plotCoord <- ROC.Coordinates(df,resp, respProb, p, n, i)
    #print(plotCoord)
    dfForPlot <-  data.frame(rbind(dfForPlot,data.frame(plotCoord)))
  }
  plot(x=dfForPlot$fpr,y=dfForPlot$tpr,lwd=2, type="l", xlab="FPR", ylab="TPR", main=plotTitle)
  lines(x=c(0, 1), y=c(0, 1), col="black", lwd=1)
  return (dfForPlot)
  
}


ROC.Coordinates <- function(df, resp, respProb, p, n, threshold)
{
  pred <- rep(n,length(df[,1]))
  dfNew <- df
  pred[which(df[respProb]>=threshold)]  <- p
  dfNew$pred <- as.factor(pred)
  
  sensitivity <- Sensitivity(df = dfNew, predResp = "pred",resp = resp,p = p,n = n)
  specificity <- Specificity(df = dfNew, predResp = "pred",resp = resp,p = p,n = n)
  
  tpr <- sensitivity
  fpr <- 1 - specificity
  auc <- (sensitivity + specificity)/2
  dist <- sqrt((1-tpr)^2 + (fpr)^2)
  
  return (data.frame(threshold,fpr,tpr,auc,dist))
  
}

calculateAUC <- function(curveInfo)
{
  tpr1 <- 0
  fpr1 <- 0
  auc <- 0
  curveInfo <- curveInfo[order(-curveInfo$threshold),]
  for(i in 1:nrow(curveInfo))
  {
    tpr2 <- curveInfo[i,"tpr"]
    fpr2 <- curveInfo[i,"fpr"]
    
    auc <- auc + ((tpr1 + tpr2)/2)*(fpr2-fpr1)
    
    
    tpr1 <- tpr2
    fpr1 <- fpr2
  }
  return(auc)
}


ConfusionDataFrame <- function(tp,fp,tn,fn)
{
  cm <- data.frame( rbind(tp,fp), rbind(fn, tn))
  colnames(cm) <- c("Act-Pos","Act-Neg")
  rownames(cm) <- c("Pred-Pos","Pred-Neg")
  return(kable(cm))
}

ModelMetrics <- function(df, predResp, resp, p, n, modelName, aic)
{
  acc <- Accuracy(dfPred,"predResp","target",1,0)
  cer <- ClassificationErrorRate(dfPred,"predResp","target",1,0)
  prec <- Precision(dfPred,"predResp","target",1,0)
  sens <- Sensitivity(dfPred,"predResp","target",1,0)
  spec <- Specificity(dfPred,"predResp","target",1,0)
  f1score <- F1Score(dfPred,"predResp","target",1,0)
  roc <- ROC(dfPred,"predProb","target",1,0, seq(0,1,.01),modelName)
  
  auc <- calculateAUC(roc)
  
  bestThreshold <- roc[which(roc$auc == max(roc$auc)),][1,][,1]

  
  tp <- TP(dfPred,"predResp","target",1,0)
  tn <- TN(dfPred,"predResp","target",1,0)
  fp <- FP(dfPred,"predResp","target",1,0)
  fn <- FN(dfPred,"predResp","target",1,0)
  
  print(ConfusionDataFrame(tp,fp,tn,fn))
  
  M <- format(c(acc,cer,prec,sens,spec,f1score,auc,bestThreshold,aic), digits = 3)
  df <- data.frame("metric" = M)
  r.n <- c("accuracy", "classif.error","precision","sensitivity","specificity","f1score","auc","best.threshold", "aic")
  rownames(df) <- r.n
  colnames(df) <- modelName
  
  return(df)
}
```

## Model 1 - Backwards Selection Method

In the backward step selection model  The resulting AIC was . 

#TODO UPDATE THIS  

```{r backwards model selection, echo=FALSE, results='asis',  cache=TRUE, eval=TRUE}
fullModel <- lm(TARGET_FLAG ~ KIDSDRIV + AGE + HOMEKIDS + YOJ + I(INCOME^(1/4)) + PARENT1 + I(sqrt(HOME_VAL)) + MSTATUS + SEX + EDUCATION + JOB + TRAVTIME + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + RED_CAR + I(log1p(OLDCLAIM)) + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + URBANICITY, data = wimputedDfTr)
backFit <- fit <- glm(step(fullModel, direction = "backward", trace = F))
stargazer(fit, header = FALSE)
```

### Model Metrics for Backwards Selection

We first use an established threshold of .50 to determine our best possible threshold. 

#TODO Add Model Metrics

\newpage

### Multicollinearity for Backwards Selection

```{r, echo=FALSE, results='asis',  cache=TRUE}
dfVifFit <- setDT(as.data.frame(car::vif(backFit)), keep.rownames = TRUE)[]
#colnames(dfVifFit) <- c("variables", "VIF", "DIF", "$\\GVIF^(\\frac{1}{(2*Df)})$")
kable(dfVifFit, align = c("l", "c", "c", "c"))
```

## Model 2 - Forwards Selection Method


```{r forward selection method, echo=FALSE, results='markup', cache=TRUE}
step(lm(TARGET_FLAG~1, data=wimputedDfTr), direction = "forward", scope = ~TARGET_FLAG ~ KIDSDRIV + AGE + HOMEKIDS + YOJ + I(INCOME^(1/4)) + PARENT1 + I(sqrt(HOME_VAL)) + MSTATUS + SEX + EDUCATION + JOB + TRAVTIME + CAR_USE + BLUEBOOK + TIF + CAR_TYPE + RED_CAR + I(log1p(OLDCLAIM)) + CLM_FREQ + REVOKED + MVR_PTS + CAR_AGE + URBANICITY) 

```

```{r, echo = FALSE, results='asis',  cache=TRUE}
forwardFit <- fit <- glm(formula = TARGET_FLAG ~ I(log1p(OLDCLAIM)) + URBANICITY + I(sqrt(HOME_VAL)) + CAR_USE + BLUEBOOK + REVOKED + MVR_PTS + PARENT1 +  I(INCOME^(1/4)) + JOB + TRAVTIME + CAR_TYPE + TIF + KIDSDRIV + MSTATUS + CAR_AGE + CLM_FREQ + YOJ, data = wimputedDfTr, family = binomial)
stargazer(fit, header = FALSE,  no.space = TRUE)
```

